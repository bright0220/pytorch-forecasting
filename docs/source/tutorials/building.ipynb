{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using custom data and implementing custom models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _new-model-tutorial:\n",
    "\n",
    "Building a new model in PyTorch Forecasting is relatively easy. Many things are taken care of automatically\n",
    "\n",
    "* Training, validation and inference is automatically handled for most models - defining the architecture and hyperparameters is sufficient\n",
    "* Dataloading, normalization, re-scaling etc. is provided by the TimeSeriesDataSet\n",
    "* Logging training progress with multiple metrics including plotting examples is automatically taken care of\n",
    "* Masking of entries if different time series have different lengths is automatic\n",
    "\n",
    "However, there a couple of things to keep in mind if you want to make full use of the package. This tutorial first demonstrates how to implement a simple model and then turns to more complicated implementation scenarios.\n",
    "\n",
    "We will answer questions such as\n",
    "\n",
    "* How to transfer an existing PyTorch implementation into PyTorch Forecasting\n",
    "* How to handle data loading and enable different length time series\n",
    "* How to define and use a custom metric\n",
    "* How to handle recurrent networks\n",
    "* How to deal with covariates\n",
    "* How to test new models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple, first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes we will choose a simple fully connected model. It takes a timeseries of size `input_size` as input and outputs a new timeseries of size `output_size`. You can think of this `input_size` encoding steps and `output_size` decoding/prediction steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class FullyConnectedModule(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # input layer\n",
    "        module_list = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden_layers):\n",
    "            module_list.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
    "        # output layer\n",
    "        module_list.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x of shape: batch_size x n_timesteps_in\n",
    "        # output of shape batch_size x n_timesteps_out\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "# test that network works as intended\n",
    "network = FullyConnectedModule(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2)\n",
    "x = torch.rand(20, 5)\n",
    "network(x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The above model is not yet a PyTorch Forecasting model but it is easy to get there. As this is a simple model, we will use the :py:class:`~pytorch_forecasting.models.base_model.BaseModel`. This base class is modified `LightningModule <https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html>`_ with pre-defined hooks for training and validating time series models. The :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` will be discussed later in this tutorial.\n",
    "\n",
    "Either way, the main requirement is for the model to have a ``forward`` method.\n",
    "\n",
    ".. automethod:: pytorch_forecasting.models.base_model.BaseModel.forward\n",
    "    :noindex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from pytorch_forecasting.models import BaseModel\n",
    "\n",
    "\n",
    "class FullyConnectedModel(BaseModel):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "        prediction = self.network(network_input)\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "\n",
    "model = FullyConnectedModel(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very basic implementation that could be readily used for training. But before we add additional features, let's first have a look how we pass data to this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing data to a model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _passing-data:\n",
    "\n",
    "Instead of having to write our own dataloader (which can be rather complicated), we can leverage PyTorch Forecasting's :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` to feed data to our model.\n",
    "In fact, PyTorch Forecasting expects us to use a :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet`.\n",
    "\n",
    "The data has to be in a specific format to be used by the :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet`. It should be in a pandas `DataFrame` and have a categorical column to identify each series and a integer column to specify the time of the record.\n",
    "\n",
    "Below, we create such a dataset with 30 different observations - 10 for 3 time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>group</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.293099</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.030545</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.207330</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443858</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.029857</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.393216</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.325816</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.055483</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.009780</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.276114</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.329291</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.037555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.054342</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.453412</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.476923</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.086252</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.489198</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.189682</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.349081</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.389614</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.225777</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.267064</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.401633</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.450546</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.313255</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.424983</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.154324</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.167481</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.062799</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.387780</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       value  group  time_idx\n",
       "0  -0.293099      0         0\n",
       "1  -0.030545      0         1\n",
       "2   0.207330      0         2\n",
       "3   0.443858      0         3\n",
       "4  -0.029857      0         4\n",
       "5  -0.393216      0         5\n",
       "6  -0.325816      0         6\n",
       "7  -0.055483      0         7\n",
       "8   0.009780      0         8\n",
       "9  -0.276114      0         9\n",
       "10 -0.329291      1         0\n",
       "11 -0.037555      1         1\n",
       "12  0.054342      1         2\n",
       "13 -0.453412      1         3\n",
       "14  0.476923      1         4\n",
       "15  0.086252      1         5\n",
       "16  0.489198      1         6\n",
       "17 -0.189682      1         7\n",
       "18 -0.349081      1         8\n",
       "19 -0.389614      1         9\n",
       "20 -0.225777      2         0\n",
       "21 -0.267064      2         1\n",
       "22  0.401633      2         2\n",
       "23  0.450546      2         3\n",
       "24  0.313255      2         4\n",
       "25  0.424983      2         5\n",
       "26 -0.154324      2         6\n",
       "27  0.167481      2         7\n",
       "28  0.062799      2         8\n",
       "29  0.387780      2         9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "test_data = pd.DataFrame(\n",
    "    dict(\n",
    "        value=np.random.rand(30) - 0.5,\n",
    "        group=np.repeat(np.arange(3), 10),\n",
    "        time_idx=np.tile(np.arange(10), 3),\n",
    "    )\n",
    ")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Converting it to a :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "# create the dataset from the pandas dataframe\n",
    "dataset = TimeSeriesDataSet(\n",
    "    test_data,\n",
    "    group_ids=[\"group\"],\n",
    "    target=\"value\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=5,\n",
    "    max_encoder_length=5,\n",
    "    min_prediction_length=2,\n",
    "    max_prediction_length=2,\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We can take a look at all the defaults and settings that were set by PyTorch Forecasting. These are all available as arguments to :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` - see its documentation for more all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_idx': 'time_idx',\n",
       " 'target': 'value',\n",
       " 'group_ids': ['group'],\n",
       " 'weight': None,\n",
       " 'max_encoder_length': 5,\n",
       " 'min_encoder_length': 5,\n",
       " 'min_prediction_idx': 0,\n",
       " 'min_prediction_length': 2,\n",
       " 'max_prediction_length': 2,\n",
       " 'static_categoricals': [],\n",
       " 'static_reals': [],\n",
       " 'time_varying_known_categoricals': [],\n",
       " 'time_varying_known_reals': [],\n",
       " 'time_varying_unknown_categoricals': [],\n",
       " 'time_varying_unknown_reals': ['value'],\n",
       " 'variable_groups': {},\n",
       " 'dropout_categoricals': [],\n",
       " 'constant_fill_strategy': {},\n",
       " 'allow_missings': False,\n",
       " 'lags': {},\n",
       " 'add_relative_time_idx': False,\n",
       " 'add_target_scales': False,\n",
       " 'add_encoder_length': False,\n",
       " 'target_normalizer': GroupNormalizer(),\n",
       " 'categorical_encoders': {'__group_id__group': NaNLabelEncoder(),\n",
       "  'group': NaNLabelEncoder()},\n",
       " 'scalers': {},\n",
       " 'randomize_length': None,\n",
       " 'predict_mode': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take a look at the output of the dataloader. It's `x` will be fed to the model's forward method, that is why it is so important to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = {'encoder_cat': tensor([], size=(4, 5, 0), dtype=torch.int64), 'encoder_cont': tensor([[[ 0.1604],\n",
      "         [-1.5196],\n",
      "         [ 1.5585],\n",
      "         [ 0.2659],\n",
      "         [ 1.5991]],\n",
      "\n",
      "        [[-0.7664],\n",
      "         [-0.9030],\n",
      "         [ 1.3094],\n",
      "         [ 1.4712],\n",
      "         [ 1.0170]],\n",
      "\n",
      "        [[-0.9030],\n",
      "         [ 1.3094],\n",
      "         [ 1.4712],\n",
      "         [ 1.0170],\n",
      "         [ 1.3866]],\n",
      "\n",
      "        [[-1.1089],\n",
      "         [-0.1437],\n",
      "         [ 0.1604],\n",
      "         [-1.5196],\n",
      "         [ 1.5585]]]), 'encoder_target': tensor([[ 0.0543, -0.4534,  0.4769,  0.0863,  0.4892],\n",
      "        [-0.2258, -0.2671,  0.4016,  0.4505,  0.3133],\n",
      "        [-0.2671,  0.4016,  0.4505,  0.3133,  0.4250],\n",
      "        [-0.3293, -0.0376,  0.0543, -0.4534,  0.4769]]), 'encoder_lengths': tensor([5, 5, 5, 5]), 'decoder_cat': tensor([], size=(4, 2, 0), dtype=torch.int64), 'decoder_cont': tensor([[[-0.6470],\n",
      "         [-1.1744]],\n",
      "\n",
      "        [[ 1.3866],\n",
      "         [-0.5300]],\n",
      "\n",
      "        [[-0.5300],\n",
      "         [ 0.5347]],\n",
      "\n",
      "        [[ 0.2659],\n",
      "         [ 1.5991]]]), 'decoder_target': tensor([[-0.1897, -0.3491],\n",
      "        [ 0.4250, -0.1543],\n",
      "        [-0.1543,  0.1675],\n",
      "        [ 0.0863,  0.4892]]), 'decoder_lengths': tensor([2, 2, 2, 2]), 'decoder_time_idx': tensor([[7, 8],\n",
      "        [5, 6],\n",
      "        [6, 7],\n",
      "        [5, 6]]), 'groups': tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]]), 'target_scale': tensor([[0.0059, 0.3022],\n",
      "        [0.0059, 0.3022],\n",
      "        [0.0059, 0.3022],\n",
      "        [0.0059, 0.3022]])}\n",
      "\n",
      "y = (tensor([[-0.1897, -0.3491],\n",
      "        [ 0.4250, -0.1543],\n",
      "        [-0.1543,  0.1675],\n",
      "        [ 0.0863,  0.4892]]), None)\n",
      "\n",
      "sizes of x =\n",
      "\tencoder_cat = torch.Size([4, 5, 0])\n",
      "\tencoder_cont = torch.Size([4, 5, 1])\n",
      "\tencoder_target = torch.Size([4, 5])\n",
      "\tencoder_lengths = torch.Size([4])\n",
      "\tdecoder_cat = torch.Size([4, 2, 0])\n",
      "\tdecoder_cont = torch.Size([4, 2, 1])\n",
      "\tdecoder_target = torch.Size([4, 2])\n",
      "\tdecoder_lengths = torch.Size([4])\n",
      "\tdecoder_time_idx = torch.Size([4, 2])\n",
      "\tgroups = torch.Size([4, 1])\n",
      "\ttarget_scale = torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "# convert the dataset to a dataloader\n",
    "dataloader = dataset.to_dataloader(batch_size=4)\n",
    "\n",
    "# and load the first batch\n",
    "x, y = next(iter(dataloader))\n",
    "print(\"x =\", x)\n",
    "print(\"\\ny =\", y)\n",
    "print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "To understand it better, we look at documentation of the :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.to_dataloader` method:\n",
    "\n",
    ".. automethod:: pytorch_forecasting.data.timeseries.TimeSeriesDataSet.to_dataloader\n",
    "    :noindex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This explains why we had to first extract the correct input in our simple `FullyConnectedModel` above before passing it to our `FullyConnectedModule`.\n",
    "As a reminder:\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    # x is a batch generated based on the TimeSeriesDataset\n",
    "    network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "    prediction = self.network(network_input)\n",
    "\n",
    "    # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "    # The parameter can be directly forwarded from the input.\n",
    "    return dict(prediction=prediction, target_scale=x[\"target_scale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a simple architecture, we can ignore most of the inputs in ``x``. You do not have to worry about moving tensors to specifc GPUs, [PyTorch Lightning](https://pytorch-lightning.readthedocs.io) will take care of this for you.\n",
    "\n",
    "Now, let's check if our model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': tensor([[ 0.1789, -0.3471],\n",
       "         [ 0.1626, -0.2557],\n",
       "         [ 0.1543, -0.2441],\n",
       "         [ 0.1247, -0.2500]], grad_fn=<AddmmBackward>),\n",
       " 'target_scale': tensor([[0.0059, 0.3022],\n",
       "         [0.0059, 0.3022],\n",
       "         [0.0059, 0.3022],\n",
       "         [0.0059, 0.3022]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "If you want to know to which group and time index (at the first prediction) the samples in the batch link to, you can find out by using :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.x_to_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_idx</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_idx  group\n",
       "0         6      0\n",
       "1         5      0\n",
       "2         5      2\n",
       "3         6      2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.x_to_index(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling datasets and models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "You might have noticed that the encoder and decoder/prediction lengths (5 and 2) are already specified in the :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` and we specified them a second time when initializing the model. This might be acceptable for such a simple model but will make it hard for users to understand how to map form the dataset to the model parameters in more complicated settings.\n",
    "This is why we should implement another method in the model: ``from_dataset()``. Typically, a user would always initialize a model from a dataset. The method is also an opportunity to validate that the dataset defined by the user is compatible with your model architecture.\n",
    "\n",
    "While the :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` and all PyTorch Forecasting metrics support different length time series, not every network architecture does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedModel(BaseModel):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "        prediction = self.network(network_input).unsqueeze(-1)\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "        assert (\n",
    "            len(dataset.time_varying_known_categoricals) == 0\n",
    "            and len(dataset.time_varying_known_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_categoricals) == 0\n",
    "            and len(dataset.static_categoricals) == 0\n",
    "            and len(dataset.static_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_reals) == 1\n",
    "            and dataset.time_varying_unknown_reals[0] == dataset.target\n",
    "        ), \"Only covariate should be the target in 'time_varying_unknown_reals'\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialize from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                 | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0  | loss                 | SMAPE                | 0     \n",
      "1  | logging_metrics      | ModuleList           | 0     \n",
      "2  | network              | FullyConnectedModule | 302   \n",
      "3  | network.sequential   | Sequential           | 302   \n",
      "4  | network.sequential.0 | Linear               | 60    \n",
      "5  | network.sequential.1 | ReLU                 | 0     \n",
      "6  | network.sequential.2 | Linear               | 110   \n",
      "7  | network.sequential.3 | ReLU                 | 0     \n",
      "8  | network.sequential.4 | Linear               | 110   \n",
      "9  | network.sequential.5 | ReLU                 | 0     \n",
      "10 | network.sequential.6 | Linear               | 22    \n",
      "---------------------------------------------------------------\n",
      "302       Trainable params\n",
      "0         Non-trainable params\n",
      "302       Total params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       SMAPE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FullyConnectedModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2)\n",
    "model.summarize(\"full\")  # print model summary\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining additional hyperparameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "So far, we have kept a wildcard ``**kwargs`` argument in the model initialization signature. We then pass these ``**kwargs`` to the :py:class:`~pytorch_forecasting.models.base_model.BaseModel` using a ``super().__init__(**kwargs)`` call. We can see which additional hyperparameters are available as they are all saved in the ``hparams`` attribute of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       SMAPE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "While not required, to give the user transparancy over these additional hyperparameters, it is worth passing them explicitly instead of implicitly in ``**kwargs``\n",
    "\n",
    "They are described in detail in the :py:class:`~pytorch_forecasting.models.base_model.BaseModel`. \n",
    "\n",
    ".. automethod:: pytorch_forecasting.models.base_model.BaseModel.__init__\n",
    "    :noindex:\n",
    "    \n",
    "You can simply copy this docstring into your model implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        BaseModel for timeseries forecasting from which to inherit from\n",
      "\n",
      "        Args:\n",
      "            log_interval (Union[int, float], optional): Batches after which predictions are logged. If < 1.0, will log\n",
      "                multiple entries per batch. Defaults to -1.\n",
      "            log_val_interval (Union[int, float], optional): batches after which predictions for validation are\n",
      "                logged. Defaults to None/log_interval.\n",
      "            learning_rate (float, optional): Learning rate. Defaults to 1e-3.\n",
      "            log_gradient_flow (bool): If to log gradient flow, this takes time and should be only done to diagnose\n",
      "                training failures. Defaults to False.\n",
      "            loss (Metric, optional): metric to optimize. Defaults to SMAPE().\n",
      "            logging_metrics (nn.ModuleList[MultiHorizonMetric]): list of metrics that are logged during training.\n",
      "                Defaults to [].\n",
      "            reduce_on_plateau_patience (int): patience after which learning rate is reduced by a factor of 10. Defaults\n",
      "                to 1000\n",
      "            reduce_on_plateau_min_lr (float): minimum learning rate for reduce on plateua learning rate scheduler.\n",
      "                Defaults to 1e-5\n",
      "            weight_decay (float): weight decay. Defaults to 0.0.\n",
      "            monotone_constaints (Dict[str, int]): dictionary of monotonicity constraints for continuous decoder\n",
      "                variables mapping\n",
      "                position (e.g. ``\"0\"`` for first position) to constraint (``-1`` for negative and ``+1`` for positive,\n",
      "                larger numbers add more weight to the constraint vs. the loss but are usually not necessary).\n",
      "                This constraint significantly slows down training. Defaults to {}.\n",
      "            output_transformer (Callable): transformer that takes network output and transforms it to prediction space.\n",
      "                Defaults to None which is equivalent to ``lambda out: out[\"prediction\"]``.\n",
      "            optimizer (str): Optimizer, \"ranger\", \"adam\" or \"adamw\". Defaults to \"ranger\".\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(BaseModel.__init__.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Classification is a common task and can be easily implemented. In fact, we only have to change the target in our :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` and adjust the number of prediction outputs to reflect the number of classes we want to predict. The changes for the :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` are marked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "      <th>group</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>0.131473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>0.571449</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>0.438259</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>0.214781</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>0.351263</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>0.749218</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>0.769624</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C</td>\n",
       "      <td>0.877673</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C</td>\n",
       "      <td>0.363937</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C</td>\n",
       "      <td>0.078857</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B</td>\n",
       "      <td>0.622963</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B</td>\n",
       "      <td>0.580132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A</td>\n",
       "      <td>0.904214</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C</td>\n",
       "      <td>0.559017</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B</td>\n",
       "      <td>0.475967</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C</td>\n",
       "      <td>0.644285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B</td>\n",
       "      <td>0.950599</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A</td>\n",
       "      <td>0.108148</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B</td>\n",
       "      <td>0.931649</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B</td>\n",
       "      <td>0.449216</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>B</td>\n",
       "      <td>0.051974</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>C</td>\n",
       "      <td>0.504904</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>A</td>\n",
       "      <td>0.666047</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>C</td>\n",
       "      <td>0.096394</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>C</td>\n",
       "      <td>0.102322</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>C</td>\n",
       "      <td>0.629819</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>C</td>\n",
       "      <td>0.706347</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>C</td>\n",
       "      <td>0.470337</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>B</td>\n",
       "      <td>0.408104</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>A</td>\n",
       "      <td>0.344363</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target     value  group  time_idx\n",
       "0       B  0.131473      0         0\n",
       "1       A  0.571449      0         1\n",
       "2       B  0.438259      0         2\n",
       "3       B  0.214781      0         3\n",
       "4       C  0.351263      0         4\n",
       "5       C  0.749218      0         5\n",
       "6       C  0.769624      0         6\n",
       "7       C  0.877673      0         7\n",
       "8       C  0.363937      0         8\n",
       "9       C  0.078857      0         9\n",
       "10      B  0.622963      1         0\n",
       "11      B  0.580132      1         1\n",
       "12      A  0.904214      1         2\n",
       "13      C  0.559017      1         3\n",
       "14      B  0.475967      1         4\n",
       "15      C  0.644285      1         5\n",
       "16      B  0.950599      1         6\n",
       "17      A  0.108148      1         7\n",
       "18      B  0.931649      1         8\n",
       "19      B  0.449216      1         9\n",
       "20      B  0.051974      2         0\n",
       "21      C  0.504904      2         1\n",
       "22      A  0.666047      2         2\n",
       "23      C  0.096394      2         3\n",
       "24      C  0.102322      2         4\n",
       "25      C  0.629819      2         5\n",
       "26      C  0.706347      2         6\n",
       "27      C  0.470337      2         7\n",
       "28      B  0.408104      2         8\n",
       "29      A  0.344363      2         9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_test_data = pd.DataFrame(\n",
    "    dict(\n",
    "        target=np.random.choice([\"A\", \"B\", \"C\"], size=30),  # CHANGING values to predict to a categorical\n",
    "        value=np.random.rand(30),  # INPUT values - see next section on covariates how to use categorical inputs\n",
    "        group=np.repeat(np.arange(3), 10),\n",
    "        time_idx=np.tile(np.arange(10), 3),\n",
    "    )\n",
    ")\n",
    "classification_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 2],\n",
       "        [1, 0],\n",
       "        [2, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "\n",
    "# create the dataset from the pandas dataframe\n",
    "classification_dataset = TimeSeriesDataSet(\n",
    "    classification_test_data,\n",
    "    group_ids=[\"group\"],\n",
    "    target=\"target\",  # SWITCHING to categorical target\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=5,\n",
    "    max_encoder_length=5,\n",
    "    min_prediction_length=2,\n",
    "    max_prediction_length=2,\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    target_normalizer=NaNLabelEncoder(),  # Use the NaNLabelEncoder to encode categorical target\n",
    ")\n",
    "\n",
    "x, y = next(iter(classification_dataset.to_dataloader(batch_size=4)))\n",
    "y[0]  # target values are encoded categories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The keyword argument ``target_normalizer`` is here redundant because the would have detected that a categorical target is used and therefore a :py:class:`~pytorch_forecasting.data.encoders.NaNLabelEncoder` is required."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Now, we need to modify our implementation of the ``FullyConnectedModel``. In particular, we have to one hyperparameters to the model: ``n_classes`` which determines how\n",
    "many classes there are to predict. Our model will produce a number for each class at each timestep each of which can be converted into probabilities by applying a softmax (over the last dimension). This means we need a total of ``n_decoder_timesteps x n_classes`` predictions. Further, we need to specify the default loss function which we choose to be :py:class:`~pytorch_forecasting.metrics.CrossEntropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                 | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0  | loss                 | SMAPE                | 0     \n",
      "1  | logging_metrics      | ModuleList           | 0     \n",
      "2  | network              | FullyConnectedModule | 346   \n",
      "3  | network.sequential   | Sequential           | 346   \n",
      "4  | network.sequential.0 | Linear               | 60    \n",
      "5  | network.sequential.1 | ReLU                 | 0     \n",
      "6  | network.sequential.2 | Linear               | 110   \n",
      "7  | network.sequential.3 | ReLU                 | 0     \n",
      "8  | network.sequential.4 | Linear               | 110   \n",
      "9  | network.sequential.5 | ReLU                 | 0     \n",
      "10 | network.sequential.6 | Linear               | 66    \n",
      "---------------------------------------------------------------\n",
      "346       Trainable params\n",
      "0         Non-trainable params\n",
      "346       Total params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       CrossEntropy()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_classes\":                  3\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         NaNLabelEncoder()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_forecasting.metrics import CrossEntropy\n",
    "\n",
    "\n",
    "class FullyConnectedClassificationModel(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        n_hidden_layers: int,\n",
    "        n_classes: int,\n",
    "        loss=CrossEntropy(),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size,\n",
    "            output_size=self.hparams.output_size * self.hparams.n_classes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        batch_size = x[\"encoder_cont\"].size(0)\n",
    "        network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "        prediction = self.network(network_input)\n",
    "        # RESHAPE output to batch_size x n_decoder_timesteps x n_classes\n",
    "        prediction = prediction.unsqueeze(-1).view(batch_size, -1, self.hparams.n_classes)\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        assert isinstance(dataset.target_normalizer, NaNLabelEncoder), \"target normalizer has to encode categories\"\n",
    "        new_kwargs = {\n",
    "            \"n_classes\": len(\n",
    "                dataset.target_normalizer.classes_\n",
    "            ),  # ADD number of classes as encoded by the target normalizer\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "        assert (\n",
    "            len(dataset.time_varying_known_categoricals) == 0\n",
    "            and len(dataset.time_varying_known_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_categoricals) == 0\n",
    "            and len(dataset.static_categoricals) == 0\n",
    "            and len(dataset.static_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_reals) == 1\n",
    "        ), \"Only covariate should be in 'time_varying_unknown_reals'\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)\n",
    "\n",
    "\n",
    "model = FullyConnectedClassificationModel.from_dataset(classification_dataset, hidden_size=10, n_hidden_layers=2)\n",
    "model.summarize(\"full\")\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# passing x through model\n",
    "model(x)[\"prediction\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting multiple targets at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model to predict multiple targets simulateneously is not difficult to implement. We can even employ mixed targets, i.e. a mix of categorical and continous targets. The first step is to use define a dataframe with multiple targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "      <th>group</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708038</td>\n",
       "      <td>0.892561</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996168</td>\n",
       "      <td>0.224535</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.435526</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.722935</td>\n",
       "      <td>0.808656</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.460630</td>\n",
       "      <td>0.330108</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.211656</td>\n",
       "      <td>0.097367</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.096825</td>\n",
       "      <td>0.700904</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.179476</td>\n",
       "      <td>0.608772</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.332332</td>\n",
       "      <td>0.780481</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.566909</td>\n",
       "      <td>0.180606</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.130564</td>\n",
       "      <td>0.849136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.748018</td>\n",
       "      <td>0.732962</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.269611</td>\n",
       "      <td>0.949658</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.687084</td>\n",
       "      <td>0.018383</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.201123</td>\n",
       "      <td>0.981318</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.622923</td>\n",
       "      <td>0.657491</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.645520</td>\n",
       "      <td>0.017540</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.971075</td>\n",
       "      <td>0.260421</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.480377</td>\n",
       "      <td>0.557908</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.267365</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.015944</td>\n",
       "      <td>0.697146</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.024305</td>\n",
       "      <td>0.485432</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.817685</td>\n",
       "      <td>0.215023</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.496138</td>\n",
       "      <td>0.520177</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.570057</td>\n",
       "      <td>0.498162</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.369491</td>\n",
       "      <td>0.974443</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.622546</td>\n",
       "      <td>0.275579</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.790324</td>\n",
       "      <td>0.062325</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.487346</td>\n",
       "      <td>0.422077</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.433810</td>\n",
       "      <td>0.065927</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target1   target2  group  time_idx\n",
       "0   0.708038  0.892561      0         0\n",
       "1   0.996168  0.224535      0         1\n",
       "2   0.435526  0.967664      0         2\n",
       "3   0.722935  0.808656      0         3\n",
       "4   0.460630  0.330108      0         4\n",
       "5   0.211656  0.097367      0         5\n",
       "6   0.096825  0.700904      0         6\n",
       "7   0.179476  0.608772      0         7\n",
       "8   0.332332  0.780481      0         8\n",
       "9   0.566909  0.180606      0         9\n",
       "10  0.130564  0.849136      1         0\n",
       "11  0.748018  0.732962      1         1\n",
       "12  0.269611  0.949658      1         2\n",
       "13  0.687084  0.018383      1         3\n",
       "14  0.201123  0.981318      1         4\n",
       "15  0.622923  0.657491      1         5\n",
       "16  0.645520  0.017540      1         6\n",
       "17  0.971075  0.260421      1         7\n",
       "18  0.480377  0.557908      1         8\n",
       "19  0.267365  0.005841      1         9\n",
       "20  0.015944  0.697146      2         0\n",
       "21  0.024305  0.485432      2         1\n",
       "22  0.817685  0.215023      2         2\n",
       "23  0.496138  0.520177      2         3\n",
       "24  0.570057  0.498162      2         4\n",
       "25  0.369491  0.974443      2         5\n",
       "26  0.622546  0.275579      2         6\n",
       "27  0.790324  0.062325      2         7\n",
       "28  0.487346  0.422077      2         8\n",
       "29  0.433810  0.065927      2         9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_target_test_data = pd.DataFrame(\n",
    "    dict(\n",
    "        target1=np.random.rand(30),\n",
    "        target2=np.random.rand(30),\n",
    "        group=np.repeat(np.arange(3), 10),\n",
    "        time_idx=np.tile(np.arange(10), 3),\n",
    "    )\n",
    ")\n",
    "multi_target_test_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We can then simply pass a list to ``target`` keyword of the :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet`. The class will choose reasonable defaults for normalizing the targets but we can also specify the normalizer explicitly by assigning an instance of :py:class`~pytorch_forecasting.data.encoders.MultiNormalizer` to the ``target_normalizer`` keyword - for fun, lets use different ways of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.2117, 0.0968],\n",
       "         [0.0968, 0.1795],\n",
       "         [0.4804, 0.2674],\n",
       "         [0.6455, 0.9711]]),\n",
       " tensor([[0.0974, 0.7009],\n",
       "         [0.7009, 0.6088],\n",
       "         [0.5579, 0.0058],\n",
       "         [0.0175, 0.2604]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_forecasting.data.encoders import EncoderNormalizer, MultiNormalizer, TorchNormalizer\n",
    "\n",
    "# create the dataset from the pandas dataframe\n",
    "multi_target_dataset = TimeSeriesDataSet(\n",
    "    multi_target_test_data,\n",
    "    group_ids=[\"group\"],\n",
    "    target=[\"target1\", \"target2\"],  # USING two targets\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=5,\n",
    "    max_encoder_length=5,\n",
    "    min_prediction_length=2,\n",
    "    max_prediction_length=2,\n",
    "    time_varying_unknown_reals=[\"target1\", \"target2\"],\n",
    "    target_normalizer=MultiNormalizer(\n",
    "        [EncoderNormalizer(), TorchNormalizer()]\n",
    "    ),  # Use the NaNLabelEncoder to encode categorical target\n",
    ")\n",
    "\n",
    "x, y = next(iter(multi_target_dataset.to_dataloader(batch_size=4)))\n",
    "y[0]  # target values are a list of targets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Using multiple targets leads to a slightly different ``x`` and ``y`` of the :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet`'s dataloader.\n",
    "``y`` is still a tuple of target and weight but the target is now a list of tensors. So is the ``target_scale``, the ``encoder_target`` and the ``decoder_target`` in ``x``.\n",
    "\n",
    "For this reason not every model is automatically suited to deal with multiple targets. However, it is (very often) fairly simple to extend a model to output a list of tensors (for each target) as opposed to just one tensor (for one target). We will now modify our ``FullyConnectedModel`` to work with one or more targets.\n",
    "\n",
    "As we use multiple targets, we need to define a loss function that can handle them. The :py:class:`~pytorch_forecasting.metrics.MultiLoss` is exactly built for that purpose. It also allows weighing the losses differently. Soley for demonstration purposes, we decide to optimize the mean absolute error for the first and the symmetric mean average percentage error for the second target. We weight the error on the first target double as high as the error on the second target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                 | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0  | loss                 | MultiLoss            | 0     \n",
      "1  | logging_metrics      | ModuleList           | 0     \n",
      "2  | network              | FullyConnectedModule | 374   \n",
      "3  | network.sequential   | Sequential           | 374   \n",
      "4  | network.sequential.0 | Linear               | 110   \n",
      "5  | network.sequential.1 | ReLU                 | 0     \n",
      "6  | network.sequential.2 | Linear               | 110   \n",
      "7  | network.sequential.3 | ReLU                 | 0     \n",
      "8  | network.sequential.4 | Linear               | 110   \n",
      "9  | network.sequential.5 | ReLU                 | 0     \n",
      "10 | network.sequential.6 | Linear               | 44    \n",
      "---------------------------------------------------------------\n",
      "374       Trainable params\n",
      "0         Non-trainable params\n",
      "374       Total params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       MultiLoss(2 * MAE(), SMAPE())\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         MultiNormalizer(normalizers=[EncoderNormalizer(), TorchNormalizer()])\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"target_sizes\":               [1, 1]\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, MultiLoss\n",
    "from pytorch_forecasting.utils import to_list\n",
    "\n",
    "\n",
    "class FullyConnectedMultiTargetModel(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        n_hidden_layers: int,\n",
    "        target_sizes: Union[int, List[int]] = [],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size * len(to_list(self.hparams.target_sizes)),\n",
    "            output_size=self.hparams.output_size * sum(to_list(self.hparams.target_sizes)),\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        batch_size = x[\"encoder_cont\"].size(0)\n",
    "        network_input = x[\"encoder_cont\"].view(batch_size, -1)\n",
    "        prediction = self.network(network_input)\n",
    "        # RESHAPE output to batch_size x n_decoder_timesteps x sum_of_target_sizes\n",
    "        prediction = prediction.unsqueeze(-1).view(batch_size, self.hparams.output_size, sum(self.hparams.target_sizes))\n",
    "        # RESHAPE into list of batch_size x n_decoder_timesteps x target_sizes[i] where i=1..len(target_sizes)\n",
    "        stops = np.cumsum(self.hparams.target_sizes)\n",
    "        starts = stops - self.hparams.target_sizes\n",
    "        prediction = [prediction[..., start:stop] for start, stop in zip(starts, stops)]\n",
    "        if isinstance(self.hparams.target_sizes, int):  # only one target\n",
    "            prediction = prediction[0]\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        # By default only handle targets of size one here, categorical targets would be of larger size\n",
    "        new_kwargs = {\n",
    "            \"target_sizes\": [1] * len(to_list(dataset.target)),\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "        assert (\n",
    "            len(dataset.time_varying_known_categoricals) == 0\n",
    "            and len(dataset.time_varying_known_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_categoricals) == 0\n",
    "            and len(dataset.static_categoricals) == 0\n",
    "            and len(dataset.static_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_reals)\n",
    "            == len(dataset.target_names)  # Expect as as many unknown reals as targets\n",
    "        ), \"Only covariate should be in 'time_varying_unknown_reals'\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)\n",
    "\n",
    "\n",
    "model = FullyConnectedMultiTargetModel.from_dataset(\n",
    "    multi_target_dataset,\n",
    "    hidden_size=10,\n",
    "    n_hidden_layers=2,\n",
    "    loss=MultiLoss(metrics=[MAE(), SMAPE()], weights=[2.0, 1.0]),\n",
    ")\n",
    "model.summarize(\"full\")\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pass some data through our model and calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': [tensor([[[0.1955],\n",
       "           [0.0778]],\n",
       "  \n",
       "          [[0.2524],\n",
       "           [0.1098]],\n",
       "  \n",
       "          [[0.3482],\n",
       "           [0.1651]],\n",
       "  \n",
       "          [[0.2962],\n",
       "           [0.1373]]], grad_fn=<SliceBackward>),\n",
       "  tensor([[[ 0.1001],\n",
       "           [-0.2182]],\n",
       "  \n",
       "          [[ 0.1025],\n",
       "           [-0.2187]],\n",
       "  \n",
       "          [[ 0.1026],\n",
       "           [-0.2329]],\n",
       "  \n",
       "          [[ 0.1262],\n",
       "           [-0.1787]]], grad_fn=<SliceBackward>)],\n",
       " 'target_scale': [tensor([[0.6647, 0.2287],\n",
       "          [0.5654, 0.3014],\n",
       "          [0.6255, 0.2756],\n",
       "          [0.5058, 0.2519]]),\n",
       "  tensor([[0.4946, 0.3242],\n",
       "          [0.4946, 0.3242],\n",
       "          [0.4946, 0.3242],\n",
       "          [0.4946, 0.3242]], dtype=torch.float64)]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9705, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model.transform_output(\n",
    "    out\n",
    ")  # the model's transform_output method re-scales/de-normalizes the predictions to into the real target space\n",
    "model.loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using covariates"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Now that we have established the basics, we can move on to more advanced use cases, e.g. how can we make use of covariates - static and continuous alike. We can leverage the :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` for this. The difference to the :py:class:`~pytorch_forecasting.models.base_model.BaseModel` is a :py:meth:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates.from_dataset` method that pre-defines hyperparameters for architectures with covariates.\n",
    "\n",
    ".. autoclass:: pytorch_forecasting.models.base_model.BaseModelWithCovariates\n",
    "    :noindex:\n",
    "    :members: from_dataset\n",
    "    \n",
    "\n",
    "Here is a from the BaseModelWithCovariates docstring to copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Model with additional methods using covariates.\n",
      "\n",
      "    Assumes the following hyperparameters:\n",
      "\n",
      "    Args:\n",
      "        static_categoricals (List[str]): names of static categorical variables\n",
      "        static_reals (List[str]): names of static continuous variables\n",
      "        time_varying_categoricals_encoder (List[str]): names of categorical variables for encoder\n",
      "        time_varying_categoricals_decoder (List[str]): names of categorical variables for decoder\n",
      "        time_varying_reals_encoder (List[str]): names of continuous variables for encoder\n",
      "        time_varying_reals_decoder (List[str]): names of continuous variables for decoder\n",
      "        x_reals (List[str]): order of continuous variables in tensor passed to forward function\n",
      "        x_categoricals (List[str]): order of categorical variables in tensor passed to forward function\n",
      "        embedding_sizes (Dict[str, Tuple[int, int]]): dictionary mapping categorical variables to tuple of integers\n",
      "            where the first integer denotes the number of categorical classes and the second the embedding size\n",
      "        embedding_labels (Dict[str, List[str]]): dictionary mapping (string) indices to list of categorical labels\n",
      "        embedding_paddings (List[str]): names of categorical variables for which label 0 is always mapped to an\n",
      "             embedding vector filled with zeros\n",
      "        categorical_groups (Dict[str, List[str]]): dictionary of categorical variables that are grouped together and\n",
      "            can also take multiple values simultaneously (e.g. holiday during octoberfest). They should be implemented\n",
      "            as bag of embeddings\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
    "\n",
    "print(BaseModelWithCovariates.__doc__)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We will now implement the model. A helpful module is the :py:class:`~pytorch_forecasting.models.nn.embeddings.MultiEmbedding` which can be used to embed categorical features. It is compliant with he :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet`, i.e. it supports bags of embeddings that are useful for embeddings where multiple categories can occur at the same time such holidays. Again, we will create a fully-connected network. It is easy to recycle our ``FullyConnectedModule`` by simply replacing setting ``input_size`` to the number of encoder time steps times the number of features instead of simply the number of encoder time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from pytorch_forecasting.models.nn import MultiEmbedding\n",
    "\n",
    "\n",
    "class FullyConnectedModelWithCovariates(BaseModelWithCovariates):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        n_hidden_layers: int,\n",
    "        x_reals: List[str],\n",
    "        x_categoricals: List[str],\n",
    "        embedding_sizes: Dict[str, Tuple[int, int]],\n",
    "        embedding_labels: Dict[str, List[str]],\n",
    "        static_categoricals: List[str],\n",
    "        static_reals: List[str],\n",
    "        time_varying_categoricals_encoder: List[str],\n",
    "        time_varying_categoricals_decoder: List[str],\n",
    "        time_varying_reals_encoder: List[str],\n",
    "        time_varying_reals_decoder: List[str],\n",
    "        embedding_paddings: List[str],\n",
    "        categorical_groups: Dict[str, List[str]],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # create embedder - can be fed with x[\"encoder_cat\"] or x[\"decoder_cat\"] and will return\n",
    "        # dictionary of category names mapped to embeddings\n",
    "        self.input_embeddings = MultiEmbedding(\n",
    "            embedding_sizes=self.hparams.embedding_sizes,\n",
    "            categorical_groups=self.hparams.categorical_groups,\n",
    "            embedding_paddings=self.hparams.embedding_paddings,\n",
    "            x_categoricals=self.hparams.x_categoricals,\n",
    "            max_embedding_size=self.hparams.hidden_size,\n",
    "        )\n",
    "\n",
    "        # calculate the size of all concatenated embeddings + continous variables\n",
    "        n_features = sum(\n",
    "            embedding_size for classes_size, embedding_size in self.hparams.embedding_sizes.values()\n",
    "        ) + len(self.reals)\n",
    "\n",
    "        # create network that will be fed with continious variables and embeddings\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size * n_features,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        batch_size = x[\"encoder_lengths\"].size(0)\n",
    "        embeddings = self.input_embeddings(x[\"encoder_cat\"])  # returns dictionary with embedding tensors\n",
    "        network_input = torch.cat(\n",
    "            [x[\"encoder_cont\"]]\n",
    "            + [\n",
    "                emb\n",
    "                for name, emb in embeddings.items()\n",
    "                if name in self.encoder_variables or name in self.static_variables\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        prediction = self.network(network_input.view(batch_size, -1))\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We have used here additional hooks available through the :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` such as ``self.static_variables`` or ``self.encoder_variables`` that can be readily determined from the hyperparameters. See the documentation of the :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` class for all available additions to the :py:class:`~pytorch_forecasting.models.base_model.BaseModel`.\n",
    "\n",
    "When the model receives its input `x`, you can use the hyperparameters and linked to variables and the additional variables by the :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` to identify the different variables. This is important as ``x[\"encoder_cat\"].size(2) == x[\"decoder_cat\"].size(2)`` and ``x[\"encoder_cont\"].size(2) == x[\"decoder_cont\"].size(2)``. This means all variables are passed to the encoder and decoder even if some are not allowed to be used by the decoder as they are not known in the future. The order of variables in ``x[\"encoder_cont\"]`` / ``x[\"decoder_cont\"]`` and ``x[\"encoder_cat\"]`` / ``x[\"decoder_cat\"]``is determined by the hyperparameters ``x_reals`` and ``x_categoricals``. Consequently, you can idenify, for example, the position of all continuous decoder variables with ``[self.hparams.x_reals.index(name) for name in self.hparams.time_varying_reals_decoder]``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model does not make use of the known covariates in the decoder - this is obviously suboptimal but not scope of this tutorial. Anyways, let us create a new dataset with categorical variables and see how the model can be instantiated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>group</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>categorical_covariate</th>\n",
       "      <th>real_covariate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.073971</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>0.972185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.990986</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>0.705750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.890983</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "      <td>0.288414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.532790</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>0.154403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.739187</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>b</td>\n",
       "      <td>0.169346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.846289</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>b</td>\n",
       "      <td>0.428519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.396386</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>b</td>\n",
       "      <td>0.980091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.666648</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>b</td>\n",
       "      <td>0.829924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.282517</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>b</td>\n",
       "      <td>0.203935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.688395</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>b</td>\n",
       "      <td>0.645038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.742083</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>0.188957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.560686</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>0.862536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.689314</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>0.378716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.934328</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>0.252710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.017292</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>0.218257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.789946</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>0.449123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.637262</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "      <td>0.835504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.178988</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "      <td>0.644989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.664142</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>b</td>\n",
       "      <td>0.045466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.721257</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>b</td>\n",
       "      <td>0.292257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.460875</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>0.434651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.919309</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>0.692671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.210201</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "      <td>0.211042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.572374</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>0.880203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.707219</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>b</td>\n",
       "      <td>0.763475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.400850</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>0.378869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.892368</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>b</td>\n",
       "      <td>0.400466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.830922</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>b</td>\n",
       "      <td>0.748606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.038368</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "      <td>0.735625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.186857</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>a</td>\n",
       "      <td>0.242946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       value group  time_idx categorical_covariate  real_covariate\n",
       "0   0.073971     0         0                     a        0.972185\n",
       "1   0.990986     0         1                     a        0.705750\n",
       "2   0.890983     0         2                     b        0.288414\n",
       "3   0.532790     0         3                     a        0.154403\n",
       "4   0.739187     0         4                     b        0.169346\n",
       "5   0.846289     0         5                     b        0.428519\n",
       "6   0.396386     0         6                     b        0.980091\n",
       "7   0.666648     0         7                     b        0.829924\n",
       "8   0.282517     0         8                     b        0.203935\n",
       "9   0.688395     0         9                     b        0.645038\n",
       "10  0.742083     1         0                     a        0.188957\n",
       "11  0.560686     1         1                     b        0.862536\n",
       "12  0.689314     1         2                     a        0.378716\n",
       "13  0.934328     1         3                     a        0.252710\n",
       "14  0.017292     1         4                     a        0.218257\n",
       "15  0.789946     1         5                     a        0.449123\n",
       "16  0.637262     1         6                     a        0.835504\n",
       "17  0.178988     1         7                     a        0.644989\n",
       "18  0.664142     1         8                     b        0.045466\n",
       "19  0.721257     1         9                     b        0.292257\n",
       "20  0.460875     2         0                     b        0.434651\n",
       "21  0.919309     2         1                     a        0.692671\n",
       "22  0.210201     2         2                     b        0.211042\n",
       "23  0.572374     2         3                     a        0.880203\n",
       "24  0.707219     2         4                     b        0.763475\n",
       "25  0.400850     2         5                     a        0.378869\n",
       "26  0.892368     2         6                     b        0.400466\n",
       "27  0.830922     2         7                     b        0.748606\n",
       "28  0.038368     2         8                     a        0.735625\n",
       "29  0.186857     2         9                     a        0.242946"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "test_data_with_covariates = pd.DataFrame(\n",
    "    dict(\n",
    "        # as before\n",
    "        value=np.random.rand(30),\n",
    "        group=np.repeat(np.arange(3), 10),\n",
    "        time_idx=np.tile(np.arange(10), 3),\n",
    "        # now adding covariates\n",
    "        categorical_covariate=np.random.choice([\"a\", \"b\"], size=30),\n",
    "        real_covariate=np.random.rand(30),\n",
    "    )\n",
    ").astype(\n",
    "    dict(group=str)\n",
    ")  # categorical covariates have to be of string type\n",
    "test_data_with_covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                                              | Type                 | Params\n",
      "--------------------------------------------------------------------------------------------\n",
      "0  | loss                                              | SMAPE                | 0     \n",
      "1  | logging_metrics                                   | ModuleList           | 0     \n",
      "2  | input_embeddings                                  | MultiEmbedding       | 11    \n",
      "3  | input_embeddings.embeddings                       | ModuleDict           | 11    \n",
      "4  | input_embeddings.embeddings.group                 | Embedding            | 9     \n",
      "5  | input_embeddings.embeddings.categorical_covariate | Embedding            | 2     \n",
      "6  | network                                           | FullyConnectedModule | 552   \n",
      "7  | network.sequential                                | Sequential           | 552   \n",
      "8  | network.sequential.0                              | Linear               | 310   \n",
      "9  | network.sequential.1                              | ReLU                 | 0     \n",
      "10 | network.sequential.2                              | Linear               | 110   \n",
      "11 | network.sequential.3                              | ReLU                 | 0     \n",
      "12 | network.sequential.4                              | Linear               | 110   \n",
      "13 | network.sequential.5                              | ReLU                 | 0     \n",
      "14 | network.sequential.6                              | Linear               | 22    \n",
      "--------------------------------------------------------------------------------------------\n",
      "563       Trainable params\n",
      "0         Non-trainable params\n",
      "563       Total params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"categorical_groups\":                {}\n",
       "\"embedding_labels\":                  {'group': {'0': 0, '1': 1, '2': 2}, 'categorical_covariate': {'a': 0, 'b': 1}}\n",
       "\"embedding_paddings\":                []\n",
       "\"embedding_sizes\":                   {'group': [3, 3], 'categorical_covariate': [2, 1]}\n",
       "\"hidden_size\":                       10\n",
       "\"input_size\":                        5\n",
       "\"learning_rate\":                     0.001\n",
       "\"log_gradient_flow\":                 False\n",
       "\"log_interval\":                      -1\n",
       "\"log_val_interval\":                  -1\n",
       "\"logging_metrics\":                   ModuleList()\n",
       "\"loss\":                              SMAPE()\n",
       "\"monotone_constaints\":               {}\n",
       "\"n_hidden_layers\":                   2\n",
       "\"optimizer\":                         ranger\n",
       "\"output_size\":                       2\n",
       "\"output_transformer\":                GroupNormalizer(transformation='relu')\n",
       "\"reduce_on_plateau_min_lr\":          1e-05\n",
       "\"reduce_on_plateau_patience\":        1000\n",
       "\"static_categoricals\":               ['group']\n",
       "\"static_reals\":                      []\n",
       "\"time_varying_categoricals_decoder\": ['categorical_covariate']\n",
       "\"time_varying_categoricals_encoder\": ['categorical_covariate']\n",
       "\"time_varying_reals_decoder\":        ['real_covariate']\n",
       "\"time_varying_reals_encoder\":        ['real_covariate', 'value']\n",
       "\"weight_decay\":                      0.0\n",
       "\"x_categoricals\":                    ['group', 'categorical_covariate']\n",
       "\"x_reals\":                           ['real_covariate', 'value']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the dataset from the pandas dataframe\n",
    "dataset_with_covariates = TimeSeriesDataSet(\n",
    "    test_data_with_covariates,\n",
    "    group_ids=[\"group\"],\n",
    "    target=\"value\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=5,\n",
    "    max_encoder_length=5,\n",
    "    min_prediction_length=2,\n",
    "    max_prediction_length=2,\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    time_varying_known_reals=[\"real_covariate\"],\n",
    "    time_varying_known_categoricals=[\"categorical_covariate\"],\n",
    "    static_categoricals=[\"group\"],\n",
    ")\n",
    "\n",
    "model = FullyConnectedModelWithCovariates.from_dataset(dataset_with_covariates, hidden_size=10, n_hidden_layers=2)\n",
    "model.summarize(\"full\")  # print model summary\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that the model could be trained, pass a sample batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': tensor([[-0.2959, -0.2556],\n",
       "         [-0.2927, -0.2079],\n",
       "         [-0.2907, -0.2054],\n",
       "         [-0.2500, -0.1875]], grad_fn=<AddmmBackward>),\n",
       " 'target_scale': tensor([[0.5754, 0.2828],\n",
       "         [0.5754, 0.2828],\n",
       "         [0.5754, 0.2828],\n",
       "         [0.5754, 0.2828]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataset_with_covariates.to_dataloader(batch_size=4)))  # generate batch\n",
    "model(x)  # pass batch through model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an autoregressive / recurrent model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Often time series models are autoregressive, i.e. one does not make `n` predictions for all future steps in one function call but predicts ``n`` times one step ahead. PyTorch Forecasting comes with a\n",
    ":py:class:`~pytorch_forecasting.models.base_model.AutoRegressiveBaseModel` and a :py:class:`~pytorch_forecasting.models.base_model.AutoRegressiveBaseModelWithCovariates` for such models.\n",
    "\n",
    ".. autoclass:: pytorch_forecasting.models.base_model.AutoRegressiveBaseModel\n",
    "    :noindex:\n",
    "\n",
    "In this section, we will implement a simple LSTM model that could be easily extended to work with covariates. Note that because we do not handle covariates, lagged targets cannot be incorporated in this network. We use an implementation of the :py:class`~pytroch_forecasting.models.nn.rnn.LSTM` that can handle zero-length sequences but otherwise 100% mirrors the PyTorch-native implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type       | Params\n",
      "-----------------------------------------------\n",
      "0 | loss            | SMAPE      | 0     \n",
      "1 | logging_metrics | ModuleList | 0     \n",
      "2 | lstm            | LSTM       | 1.4 K \n",
      "3 | output_layer    | Linear     | 11    \n",
      "-----------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 K     Total params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"dropout\":                    0.1\n",
       "\"hidden_size\":                10\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       SMAPE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_layers\":                   2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"target\":                     value\n",
       "\"target_lags\":                {}\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils import rnn\n",
    "\n",
    "from pytorch_forecasting.models.base_model import AutoRegressiveBaseModel\n",
    "from pytorch_forecasting.models.nn import LSTM\n",
    "\n",
    "\n",
    "class LSTMModel(AutoRegressiveBaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: str,\n",
    "        target_lags: Dict[str, Dict[str, int]],\n",
    "        n_layers: int,\n",
    "        hidden_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # arguments target and target_lags are required for autoregressive models\n",
    "        # even though target_lags cannot be used without covariates\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # use version of LSTM that can handle zero-length sequences\n",
    "        self.lstm = LSTM(\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_size=1,\n",
    "            num_layers=self.hparams.n_layers,\n",
    "            dropout=self.hparams.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.hparams.hidden_size, 1)\n",
    "\n",
    "    def encode(self, x: Dict[str, torch.Tensor]):\n",
    "        # we need at least one encoding step as because the target needs to be lagged by one time step\n",
    "        # because we use the custom LSTM, we do not have to require encoder lengths of > 1\n",
    "        # but can handle lengths of >= 1\n",
    "        assert x[\"encoder_lengths\"].min() >= 1\n",
    "        input_vector = x[\"encoder_cont\"].clone()\n",
    "        # lag target by one\n",
    "        input_vector[..., self.target_positions] = torch.roll(\n",
    "            input_vector[..., self.target_positions], shifts=1, dims=1\n",
    "        )\n",
    "        input_vector = input_vector[:, 1:]  # first time step cannot be used because of lagging\n",
    "\n",
    "        # determine effective encoder_length length\n",
    "        effective_encoder_lengths = x[\"encoder_lengths\"] - 1\n",
    "        # run through LSTM network\n",
    "        _, hidden_state = self.lstm(\n",
    "            input_vector, lengths=effective_encoder_lengths, enforce_sorted=False  # passing the lengths directly\n",
    "        )  # second ouput is not needed (hidden state)\n",
    "        return hidden_state\n",
    "\n",
    "    def decode(self, x: Dict[str, torch.Tensor], hidden_state):\n",
    "        # again lag target by one\n",
    "        input_vector = x[\"decoder_cont\"].clone()\n",
    "        input_vector[..., self.target_positions] = torch.roll(\n",
    "            input_vector[..., self.target_positions], shifts=1, dims=1\n",
    "        )\n",
    "        # but this time fill in missing target from encoder_cont at the first time step instead of throwing it away\n",
    "        last_encoder_target = x[\"encoder_cont\"][\n",
    "            torch.arange(x[\"encoder_cont\"].size(0), device=x[\"encoder_cont\"].device),\n",
    "            x[\"encoder_lengths\"] - 1,\n",
    "            self.target_positions.unsqueeze(-1),\n",
    "        ].T\n",
    "        input_vector[:, 0, self.target_positions] = last_encoder_target\n",
    "\n",
    "        if self.training:  # training mode\n",
    "            lstm_output, _ = self.lstm(input_vector, hidden_state, lengths=x[\"decoder_lengths\"], enforce_sorted=False)\n",
    "\n",
    "            # transform into right shape\n",
    "            prediction = self.output_layer(lstm_output)\n",
    "\n",
    "            # predictions are not yet rescaled\n",
    "            return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "        else:  # prediction mode\n",
    "            target_pos = self.target_positions\n",
    "\n",
    "            def decode_one(idx, lagged_targets, hidden_state):\n",
    "                x = input_vector[:, [idx]]\n",
    "                # overwrite at target positions\n",
    "                x[:, 0, target_pos] = lagged_targets[-1]  # take most recent target (i.e. lag=1)\n",
    "                lstm_output, hidden_state = self.lstm(x, hidden_state)\n",
    "                # transform into right shape\n",
    "                prediction = self.output_layer(lstm_output)[:, 0]  # take first timestep\n",
    "                return prediction, hidden_state\n",
    "\n",
    "            # make predictions which are fed into next step\n",
    "            output = self.decode_autoregressive(\n",
    "                decode_one,\n",
    "                first_target=input_vector[:, 0, target_pos],\n",
    "                first_hidden_state=hidden_state,\n",
    "                target_scale=x[\"target_scale\"],\n",
    "                n_decoder_steps=input_vector.size(1),\n",
    "            )\n",
    "\n",
    "            # predictions are already rescaled\n",
    "            return dict(prediction=output, output_transformation=None, target_scale=x[\"target_scale\"])\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        hidden_state = self.encode(x)  # encode to hidden state\n",
    "        output = self.decode(x, hidden_state)  # decode leveraging hidden state\n",
    "        return output\n",
    "\n",
    "\n",
    "model = LSTMModel.from_dataset(dataset, n_layers=2, hidden_size=10)\n",
    "model.summarize(\"full\")\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We used the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.transform_output` method to apply the inverse transformation. It is also used under the hood for re-scaling/de-normalizing predictions and leverages the ``output_transformer`` to do so. The ``output_transformer`` is the ``target_normalizer`` as used in the dataset. When initializing the model from the dataset, it is automatically copied to the model.\n",
    "\n",
    "We can now check that both approaches deliver the same result in terms of prediction shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction shape in training: torch.Size([4, 2, 1])\n",
      "prediction shape in inference: torch.Size([4, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "\n",
    "print(\n",
    "    \"prediction shape in training:\", model(x)[\"prediction\"].size()\n",
    ")  # batch_size x decoder time steps x 1 (1 for one target dimension)\n",
    "model.eval()  # set model into eval mode to use autoregressive prediction\n",
    "print(\"prediction shape in inference:\", model(x)[\"prediction\"].size())  # should be the same as in training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using and defining a custom/non-trivial metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a different metric, simply pass it to the model when initializing it (preferably via the `from_dataset()` method). For example, to use mean absolute error with our `FullyConnectedModel` from the beginning of this tutorial, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       MAE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_forecasting.metrics import MAE\n",
    "\n",
    "model = FullyConnectedModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2, loss=MAE())\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some metrics might require a certain form of model prediction, e.g. quantile prediction assumes an output of shape `batch_size x n_decoder_timesteps x n_quantiles` instead of `batch_size x n_decoder_timesteps`. For the `FullyConnectedModel`, this means that we need to use a modified `FullyConnectedModule`network. Here `n_outputs` corresponds to the number of quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2, 7])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class FullyConnectedMultiOutputModule(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, n_outputs: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # input layer\n",
    "        module_list = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden_layers):\n",
    "            module_list.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
    "        # output layer\n",
    "        self.n_outputs = n_outputs\n",
    "        module_list.append(\n",
    "            nn.Linear(hidden_size, output_size * n_outputs)\n",
    "        )  # <<<<<<<< modified: replaced output_size with output_size * n_outputs\n",
    "\n",
    "        self.sequential = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x of shape: batch_size x n_timesteps_in\n",
    "        # output of shape batch_size x n_timesteps_out\n",
    "        return self.sequential(x).reshape(x.size(0), -1, self.n_outputs)  # <<<<<<<< modified: added reshape\n",
    "\n",
    "\n",
    "# test that network works as intended\n",
    "network = FullyConnectedMultiOutputModule(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2, n_outputs=7)\n",
    "x = torch.rand(20, 5)\n",
    "network(x).shape  # <<<<<<<<<< instead of shape (20, 2), returning additional dimension for quantiles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Using the above-defined ``FullyConnectedMultiOutputModule``, we could create a new model and use :py:class:`~pytorch_forecasting.metrics.QuantileLoss`. Note that you would have to align ``n_outputs`` with the number of quantiles in the :py:class:`~pytorch_forecasting.metrics.QuantileLoss` class either manually or by making use of the `from_dataset()` method. If you want to switch back to a loss on a single output such as for :py:class:`~pytorch_forecasting.metrics.MAE`, simply set the ``n_ouputs=1`` as all PyTorch Forecasting metrics can handle the additional third dimension as long as it is of size 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple case: model output can be readily converted to prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "To implement a new metric, you simply need to inherit from the :py:class:`~pytorch_forecasting.metrics.MultiHorizonMetric` and define the loss function. The :py:class:`~pytorch_forecasting.metrics.MultiHorizonMetric` handles everything from weighting to masking values for you. E.g. the mean absolute error is implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MultiHorizonMetric\n",
    "\n",
    "\n",
    "class MAE(MultiHorizonMetric):\n",
    "    def loss(self, y_pred, target):\n",
    "        loss = (self.to_prediction(y_pred) - target).abs()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "You might notice the :py:meth:`~pytorch_forecasting.metrics.Metric.to_prediction` method.  Generally speaking, it convertes ``y_pred`` to a point-prediction. By default, this means that it removes the third dimension from ``y_pred`` if there is one. For most metrics, this is exactly what you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced case: model ouptut cannot be readily converted to prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Sometimes a networks's ``forward()`` output does not trivially map to a prediction and your ``to_prediction`` needs to be implemented separately. For example, this is the case if you predict the parameters of a distribution as is the case for all classes deriving from :py:class:`~pytorch_forecasting.metrics.DistributionLoss`. In particular, this means that you need to handle training and prediction differently.\n",
    "\n",
    "We will study now the case of the :py:class:`~pytorch_forecasting.metrics.NormalDistributionLoss`. It requires us to predict the ``mean`` and the ``scale`` of the normal distribution. We can do so by leveraging our ``FullyConnectedMultiOutputModule`` class that we used for predicting multiple quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                 | Type                            | Params\n",
      "--------------------------------------------------------------------------\n",
      "0  | loss                 | NormalDistributionLoss          | 0     \n",
      "1  | logging_metrics      | ModuleList                      | 0     \n",
      "2  | network              | FullyConnectedMultiOutputModule | 324   \n",
      "3  | network.sequential   | Sequential                      | 324   \n",
      "4  | network.sequential.0 | Linear                          | 60    \n",
      "5  | network.sequential.1 | ReLU                            | 0     \n",
      "6  | network.sequential.2 | Linear                          | 110   \n",
      "7  | network.sequential.3 | ReLU                            | 0     \n",
      "8  | network.sequential.4 | Linear                          | 110   \n",
      "9  | network.sequential.5 | ReLU                            | 0     \n",
      "10 | network.sequential.6 | Linear                          | 44    \n",
      "--------------------------------------------------------------------------\n",
      "324       Trainable params\n",
      "0         Non-trainable params\n",
      "324       Total params\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       SMAPE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import copy\n",
    "\n",
    "from pytorch_forecasting.metrics import NormalDistributionLoss\n",
    "\n",
    "\n",
    "class FullyConnectedForDistributionLossModel(BaseModel):  # we inherit the `from_dataset` method\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = FullyConnectedMultiOutputModule(\n",
    "            input_size=self.hparams.input_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "            n_outputs=2,  # <<<<<<<< we predict two outputs for mean and scale of the normal distribution\n",
    "        )\n",
    "        self.loss = NormalDistributionLoss()\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "        assert (\n",
    "            len(dataset.time_varying_known_categoricals) == 0\n",
    "            and len(dataset.time_varying_known_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_categoricals) == 0\n",
    "            and len(dataset.static_categoricals) == 0\n",
    "            and len(dataset.static_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_reals) == 1\n",
    "            and dataset.time_varying_unknown_reals[0] == dataset.target\n",
    "        ), \"Only covariate should be the target in 'time_varying_unknown_reals'\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor], n_samples: int = None) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "        prediction = self.network(network_input)  # shape batch_size x n_decoder_steps x 2\n",
    "        if (\n",
    "            self.training or n_samples is None\n",
    "        ):  # training is a PyTorch variable indicating if a module is being trained (tracing gradients) or evaluated\n",
    "            assert n_samples is None, \"We need to predict parameters when training\"\n",
    "            output_transformation = True\n",
    "        else:\n",
    "            # let's sample from our distribution - first we need to scale the parameters to real space\n",
    "            scaled_parameters = self.transform_output(\n",
    "                dict(\n",
    "                    prediction=prediction,\n",
    "                    target_scale=x[\"target_scale\"],\n",
    "                )\n",
    "            )\n",
    "            # and then sample from distribution\n",
    "            prediction = self.loss.sample(scaled_parameters, n_samples)\n",
    "            output_transformation = None  # predictions are already re-scaled\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"], output_transformation=output_transformation)\n",
    "\n",
    "    def transform_output(self, out: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        # this is already implemented in pytorch forecasting but this code demonstrates the point\n",
    "        # input is forward's output\n",
    "        # depending on output, transform differently\n",
    "        if out.get(\"output_transformation\") is None:  # samples are already rescaled\n",
    "            out = out[\"prediction\"]\n",
    "        else:  # parameters need to be rescaled\n",
    "            out = self.loss.rescale_parameters(\n",
    "                out[\"prediction\"], target_scale=out[\"target_scale\"], encoder=self.output_transformer\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    def log_prediction(self, x: Dict[str, torch.Tensor], out: Dict[str, torch.Tensor], batch_idx: int) -> None:\n",
    "        if (\n",
    "            out.get(\"output_transformation\") is not None\n",
    "            and (batch_idx % self.log_interval == 0 or self.log_interval < 1.0)\n",
    "            and self.log_interval > 0\n",
    "        ):\n",
    "            out = copy(out)  # copy to avoid side-effects but do not deep copy to re-use references\n",
    "            # sample from distribution to create valid prediction\n",
    "            y_hat_detached = out[\"prediction\"].detach()\n",
    "            y_hat_samples = self.loss.sample(y_hat_detached, 100)\n",
    "            out[\"prediction\"] = y_hat_samples\n",
    "            out[\"output_transformation\"] = None\n",
    "        super().log_prediction(x, out, batch_idx=batch_idx)\n",
    "\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        x: Dict[str, torch.Tensor],\n",
    "        y: torch.Tensor,\n",
    "        out: Dict[str, torch.Tensor],\n",
    "    ) -> None:\n",
    "        # Metrics (in contrast to the training loss: distribution loss) are calculated based on point predictions.\n",
    "        # Therefore, we need to convert parameter outputs to\n",
    "        if out.get(\"output_transformation\") is not None:\n",
    "            # use distribution properties to create point prediction\n",
    "            out = copy(out)  # copy to avoid side-effects but do not deep copy to re-use references\n",
    "            y_hat_detached = out[\"prediction\"].detach()\n",
    "            y_hat_point_detached = self.loss.map_x_to_distribution(y_hat_detached).mean.unsqueeze(-1)\n",
    "            out[\"prediction\"] = y_hat_point_detached\n",
    "            out[\"output_transformation\"] = None\n",
    "        super().log_metrics(x, y, out)\n",
    "\n",
    "\n",
    "model = FullyConnectedForDistributionLossModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2)\n",
    "model.summarize(\"full\")\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "You notice that we override the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.transform_output` method. This method is responsible for rescaling the output of a network into real space. This is often trivial as the normalization of the target variable simply has to be inverted (and this is also the default). In case of distribution loss, a custom version is required.\n",
    "\n",
    "Further, the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.log_metrics` method had to be overridden because the output of the network cannot be readily used for evaluating metrics such as mean absolute error - we first need to convert them into a point prediction - here not by sampling but by analytically calculating the mean of the distribution. The same is necessary for plotting the prediction. Therefore, we need to override :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.log_prediction`. This is it - the network is fully ready for training.\n",
    "\n",
    "We can now test that the network works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter predition shape:  torch.Size([4, 2, 2])\n",
      "sample prediction shape:  torch.Size([4, 2, 200])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "\n",
    "print(\"parameter predition shape: \", model(x)[\"prediction\"].size())\n",
    "model.eval()  # set model into eval mode for sampling\n",
    "print(\"sample prediction shape: \", model(x, n_samples=200)[\"prediction\"].size())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "To run inference, you can still use the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.predict()` method, as additional arguments are passed to the network's ``forward()`` method, i.e. we can execute the following line to generate 100 traces and subsequently calculate quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2, 7])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(dataloader, n_samples=100, mode=\"quantiles\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned quantiles are here determined by the quantiles defined in the loss function and can be modified by passing a list of quantiles to at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss.quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2, 0.8]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormalDistributionLoss(quantiles=[0.2, 0.8]).quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding custom plotting and interpretation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "PyTorch Forecasting supports plotting of predictions and interpretations. The figures can also be logged as part of monitoring training progress using tensorboard. Sometimes, the output of the network cannot be directly plotted together with the actually observed time series. In these cases (such as our ``FullyConnectedForDistributionLossModel`` from the previous section), we need to fix the plotting function. Further, sometimes we want to visualize certain properties of the network every other batch or after every epoch. It is easy to make this happen with PyTorch Forecasting and the `LightningModule <https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html>`_ on which the :py:class:`~pytorch_forecasting.models.base_model.BaseModel` is based."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.log_interval` property provides a log_interval that switches automatically between the hyperparameters ``log_interval`` or ``log_val_interval`` depending if the model is in training or validation mode. If it is larger than 0, logging is enabled and if ``batch_idx % log_interval == 0`` for a batch, logging for that batch is triggered. You can even set it to a number smaller than 1 leading to multiple logging events during a single batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log often whenever an example prediction vs actuals plot is created"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "One of the easiest ways to log a figure regularly, is overriding the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.plot_prediction` method, e.g. to add something to the generated plot.\n",
    "\n",
    "In the following example, we will add an additional line indicating attention to the figure logged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_prediction(\n",
    "    self,\n",
    "    x: Dict[str, torch.Tensor],\n",
    "    out: Dict[str, torch.Tensor],\n",
    "    idx: int,\n",
    "    plot_attention: bool = True,\n",
    "    add_loss_to_title: bool = False,\n",
    "    show_future_observed: bool = True,\n",
    "    ax=None,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot actuals vs prediction and attention\n",
    "\n",
    "    Args:\n",
    "        x (Dict[str, torch.Tensor]): network input\n",
    "        out (Dict[str, torch.Tensor]): network output\n",
    "        idx (int): sample index\n",
    "        plot_attention: if to plot attention on secondary axis\n",
    "        add_loss_to_title: if to add loss to title. Default to False.\n",
    "        show_future_observed: if to show actuals for future. Defaults to True.\n",
    "        ax: matplotlib axes to plot on\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: matplotlib figure\n",
    "    \"\"\"\n",
    "    # plot prediction as normal\n",
    "    fig = super().plot_prediction(\n",
    "        x, out, idx=idx, add_loss_to_title=add_loss_to_title, show_future_observed=show_future_observed, ax=ax\n",
    "    )\n",
    "\n",
    "    # add attention on secondary axis\n",
    "    if plot_attention:\n",
    "        interpretation = self.interpret_output(out)\n",
    "        ax = fig.axes[0]\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.set_ylabel(\"Attention\")\n",
    "        encoder_length = x[\"encoder_lengths\"][idx]\n",
    "        ax2.plot(\n",
    "            torch.arange(-encoder_length, 0),\n",
    "            interpretation[\"attention\"][idx, :encoder_length].detach().cpu(),\n",
    "            alpha=0.2,\n",
    "            color=\"k\",\n",
    "        )\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "If you want to add a completely new figure, override the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.log_prediction` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log at the end of an epoch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Logging at the end of an epoch is another common use case. You might want to calculate additional results in each step and then summarize them at the end of an epoch. Here, you can override the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.step` method to calculate additional results to summarize and the ``epoch_end()`` hook provided by PyTorch Lightning.\n",
    "\n",
    "In the example below, we first calculate some interpretation result (but only if logging is enabled) and add it to the ``log`` object for later summarization. In the ``epoch_end()`` hook we take the list of saved results, and\n",
    "use the ``log_interpretation()`` method (that is defined in the model elsewhere) to log a figure to the tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(\n",
    "    self, x: Dict[str, torch.Tensor], y: torch.Tensor, batch_idx: int, **kwargs\n",
    ") -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Run for each train/val step.\n",
    "\n",
    "    Args:\n",
    "        x (Dict[str, torch.Tensor]): x as passed to the network by the dataloader\n",
    "        y (torch.Tensor): y as passed to the loss function by the dataloader\n",
    "        batch_idx (int): batch number\n",
    "        **kwargs: additional arguments to pass to the network apart from ``x``\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]: tuple where the first\n",
    "            entry is a dictionary to which additional logging results can be added for consumption in the\n",
    "            ``epoch_end`` hook and the second entry is the model's output.\n",
    "    \"\"\"\n",
    "    # extract data and run model\n",
    "    log, out = super().step(x, y, batch_idx)\n",
    "    # calculate interpretations etc for latter logging\n",
    "    if self.log_interval > 0:\n",
    "        detached_output = {name: tensor.detach() for name, tensor in out.items()}\n",
    "        interpretation = self.interpret_output(\n",
    "            detached_output,\n",
    "            reduction=\"sum\",\n",
    "            attention_prediction_horizon=0,  # attention only for first prediction horizon\n",
    "        )\n",
    "        log[\"interpretation\"] = interpretation\n",
    "    return log, out\n",
    "\n",
    "\n",
    "def epoch_end(self, outputs):\n",
    "    \"\"\"\n",
    "    Run at epoch end for training or validation\n",
    "    \"\"\"\n",
    "    if self.log_interval > 0:\n",
    "        self.log_interpretation(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log at the end of training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "A common use case is to log the final embeddings at the end of training. You can easily achieve this by levering the PyTorch Lightning ``on_fit_end()`` model hook. Override that method to log the embeddings.\n",
    "\n",
    "The follow example assumes that there is a ``input_embeddings`` is a dictionary like object of embeddings that are being trained such as the :py:class:`~pytorch_forecasting.models.nn.embeddings.MultiEmbedding` class. Further a hyperparameter ``embedding_labels`` exists (as automatically required and created by the :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_fit_end(self):\n",
    "    \"\"\"\n",
    "    run at the end of training\n",
    "    \"\"\"\n",
    "    if self.log_interval > 0:\n",
    "        for name, emb in self.input_embeddings.items():\n",
    "            labels = self.hparams.embedding_labels[name]\n",
    "            self.logger.experiment.add_embedding(\n",
    "                emb.weight.data.cpu(), metadata=labels, tag=name, global_step=self.global_step\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal testing of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing models is essential to quickly detect problems and iterate quickly. Some issues can be only identified after lengthy training but many problems show up after one or two batches. PyTorch Lightning, on which PyTorch Forecasting is built, makes it easy to set up such tests."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Every model should be trainable with some minimal dataset. Here is how:\n",
    "\n",
    "#. Define a dataset that works with the model. If it takes long to create, you can save it to disk with the :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.save` method and load it with the :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.load` method when you want to run tests. In any case, create a reasonably small dataset.\n",
    "\n",
    "#. Initialize your model with ``log_interval=1`` to test logging of plots - in particular the `plot_prediction()` method.\n",
    "\n",
    "#. Define a `Pytorch Lightning Trainer <https://pytorch-lightning.readthedocs.io/en/latest/trainer.html>`_ and initialize it with ``fast_dev_run=True``. This ensures that not full epochs but just a couple of batches are passed through the training and validation steps.\n",
    "\n",
    "#. Train your model and check that it executes.\n",
    "\n",
    "As example, we marshall the ``FullyConnectedForDistributionLossModel`` defined earlier in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).\n",
      "\n",
      "  | Name            | Type                            | Params\n",
      "--------------------------------------------------------------------\n",
      "0 | loss            | NormalDistributionLoss          | 0     \n",
      "1 | logging_metrics | ModuleList                      | 0     \n",
      "2 | network         | FullyConnectedMultiOutputModule | 324   \n",
      "--------------------------------------------------------------------\n",
      "324       Trainable params\n",
      "0         Non-trainable params\n",
      "324       Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 1/2 [00:00<00:00, 19.86it/s, loss=-0.217]\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 20.29it/s, loss=-0.217, train_loss_step=-.217, val_loss=0.517]\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00, 19.08it/s, loss=-0.217, train_loss_step=-.217, val_loss=0.517]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = FullyConnectedForDistributionLossModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2, log_interval=1)\n",
    "trainer = Trainer(fast_dev_run=True)\n",
    "trainer.fit(model, train_dataloader=dataloader, val_dataloaders=dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python37764bitbaseconda4052e86d6f894f0ea94517897490b6df"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
