{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement custom models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. _new-model-tutorial:\n",
    "\n",
    "Building a new model in PyTorch Forecasting is relatively easy. Many things are taken care of automatically\n",
    "\n",
    "* Training, validation and inference is automatically handled for most models - defining the architecture and hyperparameters is sufficient\n",
    "* Dataloading, normalization, re-scaling etc. is provided by the TimeSeriesDataSet\n",
    "* Logging training progress with multiple metrics including plotting examples is automatically taken care of\n",
    "* Masking of entries if different time series have different lengths is automatic\n",
    "\n",
    "However, there a couple of things to keep in mind if you want to make full use of the package. This tutorial first demonstrates how to implement a simple model and then turns to more complicated implementation scenarios.\n",
    "\n",
    "We will answer questions such as\n",
    "\n",
    "* How to transfer an existing PyTorch implementation into PyTorch Forecasting\n",
    "* How to handle data loading and enable different length time series\n",
    "* How to define and use a custom metric\n",
    "* How to handle recurrent networks\n",
    "* How to deal with covariates\n",
    "* How to test new models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple, first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes we will choose a simple fully connected model. It takes a timeseries of size `input_size` as input and outputs a new timeseries of size `output_size`. You can think of this `input_size` encoding steps and `output_size` decoding/prediction steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class FullyConnectedModule(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # input layer\n",
    "        module_list = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden_layers):\n",
    "            module_list.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
    "        # output layer\n",
    "        module_list.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x of shape: batch_size x n_timesteps_in\n",
    "        # output of shape batch_size x n_timesteps_out\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "# test that network works as intended\n",
    "network = FullyConnectedModule(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2)\n",
    "x = torch.rand(20, 5)\n",
    "network(x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The above model is not yet a PyTorch Forecasting model but it is easy to get there. As this is a simple model, we will use the :py:class:`~pytorch_forecasting.models.base_model.BaseModel`. This base class is modified `LightningModule <https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html>`_ with pre-defined hooks for training and validating time series models. The :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` will be discussed later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from pytorch_forecasting.models import BaseModel\n",
    "\n",
    "\n",
    "class FullyConnectedModel(BaseModel):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "        prediction = self.network(network_input)\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "\n",
    "model = FullyConnectedModel(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very basic implementation that could be readily used for training. But before we add additional features, let's first have a look how we pass data to this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing data to a model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Instead of having to write our own dataloader (which can be rather complicated), we can leverage PyTorch Forecasting's :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` to feed data to our model.\n",
    "In fact, PyTorch Forecasting expects us to use a :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet`.\n",
    "\n",
    "The data has to be in a specific format to be used by the :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet`. It should be in a pandas `DataFrame` and have a categorical column to identify each series and a integer column to specify the time of the record.\n",
    "\n",
    "Below, we create such a dataset with 30 different observations - 10 for 3 time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>group</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.346013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.498930</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.267375</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.474667</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041184</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.171251</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.371327</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006309</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.406183</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.104648</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.104551</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.000881</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.098133</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.105224</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.095972</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.038881</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.105724</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.006878</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.112956</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.241046</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.046843</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.068014</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.189603</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.069823</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.311656</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.108443</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.227587</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.012920</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.056162</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.361485</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       value  group  time_idx\n",
       "0   0.346013      0         0\n",
       "1   0.498930      0         1\n",
       "2   0.267375      0         2\n",
       "3   0.474667      0         3\n",
       "4   0.041184      0         4\n",
       "5   0.171251      0         5\n",
       "6  -0.371327      0         6\n",
       "7   0.006309      0         7\n",
       "8  -0.406183      0         8\n",
       "9   0.104648      0         9\n",
       "10 -0.104551      1         0\n",
       "11 -0.000881      1         1\n",
       "12 -0.098133      1         2\n",
       "13 -0.105224      1         3\n",
       "14 -0.095972      1         4\n",
       "15  0.038881      1         5\n",
       "16  0.105724      1         6\n",
       "17  0.006878      1         7\n",
       "18  0.112956      1         8\n",
       "19  0.241046      1         9\n",
       "20 -0.046843      2         0\n",
       "21  0.068014      2         1\n",
       "22 -0.189603      2         2\n",
       "23 -0.069823      2         3\n",
       "24  0.311656      2         4\n",
       "25 -0.108443      2         5\n",
       "26  0.227587      2         6\n",
       "27 -0.012920      2         7\n",
       "28  0.056162      2         8\n",
       "29  0.361485      2         9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "test_data = pd.DataFrame(\n",
    "    dict(\n",
    "        value=np.random.rand(30) - 0.5,\n",
    "        group=np.repeat(np.arange(3), 10),\n",
    "        time_idx=np.tile(np.arange(10), 3),\n",
    "    )\n",
    ")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Converting it to a :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "# create the dataset from the pandas dataframe\n",
    "dataset = TimeSeriesDataSet(\n",
    "    test_data,\n",
    "    group_ids=[\"group\"],\n",
    "    target=\"value\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=5,\n",
    "    max_encoder_length=5,\n",
    "    min_prediction_length=2,\n",
    "    max_prediction_length=2,\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We can take a look at all the defaults and settings that were set by PyTorch Forecasting. These are all available as arguments to :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` - see its documentation for more all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_idx': 'time_idx',\n",
       " 'target': 'value',\n",
       " 'group_ids': ['group'],\n",
       " 'weight': None,\n",
       " 'max_encoder_length': 5,\n",
       " 'min_encoder_length': 5,\n",
       " 'min_prediction_idx': 0,\n",
       " 'min_prediction_length': 2,\n",
       " 'max_prediction_length': 2,\n",
       " 'static_categoricals': [],\n",
       " 'static_reals': [],\n",
       " 'time_varying_known_categoricals': [],\n",
       " 'time_varying_known_reals': [],\n",
       " 'time_varying_unknown_categoricals': [],\n",
       " 'time_varying_unknown_reals': ['value'],\n",
       " 'variable_groups': {},\n",
       " 'dropout_categoricals': [],\n",
       " 'constant_fill_strategy': {},\n",
       " 'allow_missings': False,\n",
       " 'add_relative_time_idx': False,\n",
       " 'add_target_scales': False,\n",
       " 'add_encoder_length': False,\n",
       " 'target_normalizer': GroupNormalizer(),\n",
       " 'categorical_encoders': {'__group_id__group': NaNLabelEncoder(),\n",
       "  'group': NaNLabelEncoder()},\n",
       " 'scalers': {'value': GroupNormalizer()},\n",
       " 'randomize_length': None,\n",
       " 'predict_mode': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take a look at the output of the dataloader. It's `x` will be fed to the model's forward method, that is why it is so important to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = {'encoder_cat': tensor([], size=(4, 5, 0), dtype=torch.int64), 'encoder_cont': tensor([[[-1.1748],\n",
      "         [-0.6134],\n",
      "         [ 1.1748],\n",
      "         [-0.7944],\n",
      "         [ 0.7807]],\n",
      "\n",
      "        [[-0.7793],\n",
      "         [-0.7359],\n",
      "         [-0.1038],\n",
      "         [ 0.2095],\n",
      "         [-0.2538]],\n",
      "\n",
      "        [[-0.7461],\n",
      "         [-0.7793],\n",
      "         [-0.7359],\n",
      "         [-0.1038],\n",
      "         [ 0.2095]],\n",
      "\n",
      "        [[ 2.0526],\n",
      "         [ 0.9672],\n",
      "         [ 1.9389],\n",
      "         [-0.0930],\n",
      "         [ 0.5167]]]), 'encoder_target': tensor([[-0.1896, -0.0698,  0.3117, -0.1084,  0.2276],\n",
      "        [-0.1052, -0.0960,  0.0389,  0.1057,  0.0069],\n",
      "        [-0.0981, -0.1052, -0.0960,  0.0389,  0.1057],\n",
      "        [ 0.4989,  0.2674,  0.4747,  0.0412,  0.1713]]), 'encoder_lengths': tensor([5, 5, 5, 5]), 'decoder_cat': tensor([], size=(4, 2, 0), dtype=torch.int64), 'decoder_cont': tensor([[[-0.3466],\n",
      "         [-0.0228]],\n",
      "\n",
      "        [[ 0.2434],\n",
      "         [ 0.8438]],\n",
      "\n",
      "        [[-0.2538],\n",
      "         [ 0.2434]],\n",
      "\n",
      "        [[-2.0266],\n",
      "         [-0.2565]]]), 'decoder_target': tensor([[-0.0129,  0.0562],\n",
      "        [ 0.1130,  0.2410],\n",
      "        [ 0.0069,  0.1130],\n",
      "        [-0.3713,  0.0063]]), 'decoder_lengths': tensor([2, 2, 2, 2]), 'decoder_time_idx': tensor([[7, 8],\n",
      "        [8, 9],\n",
      "        [7, 8],\n",
      "        [6, 7]]), 'groups': tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]]), 'target_scale': tensor([[0.0610, 0.2133],\n",
      "        [0.0610, 0.2133],\n",
      "        [0.0610, 0.2133],\n",
      "        [0.0610, 0.2133]])}\n",
      "\n",
      "y = tensor([[-0.0129,  0.0562],\n",
      "        [ 0.1130,  0.2410],\n",
      "        [ 0.0069,  0.1130],\n",
      "        [-0.3713,  0.0063]])\n",
      "\n",
      "sizes of x =\n",
      "\tencoder_cat = torch.Size([4, 5, 0])\n",
      "\tencoder_cont = torch.Size([4, 5, 1])\n",
      "\tencoder_target = torch.Size([4, 5])\n",
      "\tencoder_lengths = torch.Size([4])\n",
      "\tdecoder_cat = torch.Size([4, 2, 0])\n",
      "\tdecoder_cont = torch.Size([4, 2, 1])\n",
      "\tdecoder_target = torch.Size([4, 2])\n",
      "\tdecoder_lengths = torch.Size([4])\n",
      "\tdecoder_time_idx = torch.Size([4, 2])\n",
      "\tgroups = torch.Size([4, 1])\n",
      "\ttarget_scale = torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "# convert the dataset to a dataloader\n",
    "dataloader = dataset.to_dataloader(batch_size=4)\n",
    "\n",
    "# and load the first batch\n",
    "x, y = next(iter(dataloader))\n",
    "print(\"x =\", x)\n",
    "print(\"\\ny =\", y)\n",
    "print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "To understand it better, we look at documentation of the :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.to_dataloader` method:\n",
    "\n",
    ".. automethod:: pytorch_forecasting.data.timeseries.TimeSeriesDataSet.to_dataloader\n",
    "    :noindex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This explains why we had to first extract the correct input in our simple `FullyConnectedModel` above before passing it to our `FullyConnectedModule`.\n",
    "As a reminder:\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    # x is a batch generated based on the TimeSeriesDataset\n",
    "    network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "    prediction = self.network(network_input)\n",
    "\n",
    "    # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "    # The parameter can be directly forwarded from the input.\n",
    "    return dict(prediction=prediction, target_scale=x[\"target_scale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a simple architecture, we can ignore most of the inputs in ``x``. You do not have to worry about moving tensors to specifc GPUs, [PyTorch Lightning](https://pytorch-lightning.readthedocs.io) will take care of this for you.\n",
    "\n",
    "Now, let's check if our model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': tensor([[-0.2281, -0.2557],\n",
       "         [-0.2806, -0.3446],\n",
       "         [-0.2609, -0.3056],\n",
       "         [-0.1920, -0.1979]], grad_fn=<AddmmBackward>),\n",
       " 'target_scale': tensor([[0.0610, 0.2133],\n",
       "         [0.0610, 0.2133],\n",
       "         [0.0610, 0.2133],\n",
       "         [0.0610, 0.2133]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "If you want to know to which group and time index (at the first prediction) the samples in the batch link to, you can find out by using :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.x_to_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_idx</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_idx  group\n",
       "0         8      0\n",
       "1         6      1\n",
       "2         8      1\n",
       "3         6      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.x_to_index(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling datasets and models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "You might have noticed that the encoder and decoder/prediction lengths (5 and 2) are already specified in the :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` and we specified them a second time when initializing the model. This might be acceptable for such a simple model but will make it hard for users to understand how to map form the dataset to the model parameters in more complicated settings.\n",
    "This is why we should implement another method in the model: ``from_dataset()``. Typically, a user would always initialize a model from a dataset. The method is also an opportunity to validate that the dataset defined by the user is compatible with your model architecture.\n",
    "\n",
    "While the :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet` and all PyTorch Forecasting metrics support different length time series, not every network architecture does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedModel(BaseModel):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "        prediction = self.network(network_input).unsqueeze(-1)\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "        assert (\n",
    "            len(dataset.time_varying_known_categoricals) == 0\n",
    "            and len(dataset.time_varying_known_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_categoricals) == 0\n",
    "            and len(dataset.static_categoricals) == 0\n",
    "            and len(dataset.static_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_reals) == 1\n",
    "            and dataset.time_varying_unknown_reals[0] == dataset.target\n",
    "        ), \"Only covariate should be the target in 'time_varying_unknown_reals'\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialize from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                 | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0  | loss                 | SMAPE                | 0     \n",
      "1  | logging_metrics      | ModuleList           | 0     \n",
      "2  | network              | FullyConnectedModule | 302   \n",
      "3  | network.sequential   | Sequential           | 302   \n",
      "4  | network.sequential.0 | Linear               | 60    \n",
      "5  | network.sequential.1 | ReLU                 | 0     \n",
      "6  | network.sequential.2 | Linear               | 110   \n",
      "7  | network.sequential.3 | ReLU                 | 0     \n",
      "8  | network.sequential.4 | Linear               | 110   \n",
      "9  | network.sequential.5 | ReLU                 | 0     \n",
      "10 | network.sequential.6 | Linear               | 22    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       SMAPE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FullyConnectedModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2)\n",
    "model.summarize(\"full\")  # print model summary\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining additional hyperparameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "So far, we have kept a wildcard ``**kwargs`` argument in the model initialization signature. We then pass these ``**kwargs`` to the :py:class:`~pytorch_forecasting.models.base_model.BaseModel` using a ``super().__init__(**kwargs)`` call. We can see which additional hyperparameters are available as they are all saved in the ``hparams`` attribute of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       SMAPE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "While not required, to give the user transparancy over these additional hyperparameters, it is worth passing them explicitly instead of implicitly in ``**kwargs``\n",
    "\n",
    "They are described in detail in the :py:class:`~pytorch_forecasting.models.base_model.BaseModel`. \n",
    "\n",
    ".. automethod:: pytorch_forecasting.models.base_model.BaseModel.__init__\n",
    "    :noindex:\n",
    "    \n",
    "You can simply copy this docstring into your model implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        BaseModel for timeseries forecasting from which to inherit from\n",
      "\n",
      "        Args:\n",
      "            log_interval (Union[int, float], optional): Batches after which predictions are logged. If < 1.0, will log\n",
      "                multiple entries per batch. Defaults to -1.\n",
      "            log_val_interval (Union[int, float], optional): batches after which predictions for validation are\n",
      "                logged. Defaults to None/log_interval.\n",
      "            learning_rate (float, optional): Learning rate. Defaults to 1e-3.\n",
      "            log_gradient_flow (bool): If to log gradient flow, this takes time and should be only done to diagnose\n",
      "                training failures. Defaults to False.\n",
      "            loss (Metric, optional): metric to optimize. Defaults to SMAPE().\n",
      "            logging_metrics (nn.ModuleList[MultiHorizonMetric]): list of metrics that are logged during training.\n",
      "                Defaults to [].\n",
      "            reduce_on_plateau_patience (int): patience after which learning rate is reduced by a factor of 10. Defaults\n",
      "                to 1000\n",
      "            reduce_on_plateau_min_lr (float): minimum learning rate for reduce on plateua learning rate scheduler.\n",
      "                Defaults to 1e-5\n",
      "            weight_decay (float): weight decay. Defaults to 0.0.\n",
      "            monotone_constaints (Dict[str, int]): dictionary of monotonicity constraints for continuous decoder\n",
      "                variables mapping\n",
      "                position (e.g. ``\"0\"`` for first position) to constraint (``-1`` for negative and ``+1`` for positive,\n",
      "                larger numbers add more weight to the constraint vs. the loss but are usually not necessary).\n",
      "                This constraint significantly slows down training. Defaults to {}.\n",
      "            output_transformer (Callable): transformer that takes network output and transforms it to prediction space.\n",
      "                Defaults to None which is equivalent to ``lambda out: out[\"prediction\"]``.\n",
      "            optimizer (str): Optimizer, \"ranger\", \"adam\" or \"adamw\". Defaults to \"ranger\".\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(BaseModel.__init__.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using covariates"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Now that we have established the basics, we can move on to more advanced use cases, e.g. how can we make use of covariates - static and continuous alike. We can leverage the :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` for this. The difference to the :py:class:`~pytorch_forecasting.models.base_model.BaseModel` is a :py:meth:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates.from_dataset` method that pre-defines hyperparameters for architectures with covariates.\n",
    "\n",
    ".. autoclass:: pytorch_forecasting.models.base_model.BaseModelWithCovariates\n",
    "    :noindex:\n",
    "    :members: from_dataset\n",
    "    \n",
    "\n",
    "Here is a from the BaseModelWithCovariates docstring to copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Model with additional methods using covariates.\n",
      "\n",
      "    Assumes the following hyperparameters:\n",
      "\n",
      "    Args:\n",
      "        static_categoricals (List[str]): names of static categorical variables\n",
      "        static_reals (List[str]): names of static continuous variables\n",
      "        time_varying_categoricals_encoder (List[str]): names of categorical variables for encoder\n",
      "        time_varying_categoricals_decoder (List[str]): names of categorical variables for decoder\n",
      "        time_varying_reals_encoder (List[str]): names of continuous variables for encoder\n",
      "        time_varying_reals_decoder (List[str]): names of continuous variables for decoder\n",
      "        x_reals (List[str]): order of continuous variables in tensor passed to forward function\n",
      "        x_categoricals (List[str]): order of categorical variables in tensor passed to forward function\n",
      "        embedding_sizes (Dict[str, Tuple[int, int]]): dictionary mapping categorical variables to tuple of integers\n",
      "            where the first integer denotes the number of categorical classes and the second the embedding size\n",
      "        embedding_labels (Dict[str, List[str]]): dictionary mapping (string) indices to list of categorical labels\n",
      "        embedding_paddings (List[str]): names of categorical variables for which label 0 is always mapped to an\n",
      "             embedding vector filled with zeros\n",
      "        categorical_groups (Dict[str, List[str]]): dictionary of categorical variables that are grouped together and\n",
      "            can also take multiple values simultaneously (e.g. holiday during octoberfest). They should be implemented\n",
      "            as bag of embeddings\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
    "\n",
    "print(BaseModelWithCovariates.__doc__)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We will now implement the model. A helpful module is the :py:class:`~pytorch_forecasting.models.nn.embeddings.MultiEmbedding` which can be used to embed categorical features. It is compliant with he :py:class:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet`, i.e. it supports bags of embeddings that are useful for embeddings where multiple categories can occur at the same time such holidays. Again, we will create a fully-connected network. It is easy to recycle our ``FullyConnectedModule`` by simply replacing setting ``input_size`` to the number of encoder time steps times the number of features instead of simply the number of encoder time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from pytorch_forecasting.models.nn import MultiEmbedding\n",
    "\n",
    "\n",
    "class FullyConnectedModelWithCovariates(BaseModelWithCovariates):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        hidden_size: int,\n",
    "        n_hidden_layers: int,\n",
    "        x_reals: List[str],\n",
    "        x_categoricals: List[str],\n",
    "        embedding_sizes: Dict[str, Tuple[int, int]],\n",
    "        embedding_labels: Dict[str, List[str]],\n",
    "        static_categoricals: List[str],\n",
    "        static_reals: List[str],\n",
    "        time_varying_categoricals_encoder: List[str],\n",
    "        time_varying_categoricals_decoder: List[str],\n",
    "        time_varying_reals_encoder: List[str],\n",
    "        time_varying_reals_decoder: List[str],\n",
    "        embedding_paddings: List[str],\n",
    "        categorical_groups: Dict[str, List[str]],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # create embedder - can be fed with x[\"encoder_cat\"] or x[\"decoder_cat\"] and will return\n",
    "        # dictionary of category names mapped to embeddings\n",
    "        self.input_embeddings = MultiEmbedding(\n",
    "            embedding_sizes=self.hparams.embedding_sizes,\n",
    "            categorical_groups=self.hparams.categorical_groups,\n",
    "            embedding_paddings=self.hparams.embedding_paddings,\n",
    "            x_categoricals=self.hparams.x_categoricals,\n",
    "            max_embedding_size=self.hparams.hidden_size,\n",
    "        )\n",
    "\n",
    "        # calculate the size of all concatenated embeddings + continous variables\n",
    "        n_features = sum(\n",
    "            embedding_size for classes_size, embedding_size in self.hparams.embedding_sizes.values()\n",
    "        ) + len(self.reals)\n",
    "\n",
    "        # create network that will be fed with continious variables and embeddings\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size * n_features,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        batch_size = x[\"encoder_lengths\"].size(0)\n",
    "        embeddings = self.input_embeddings(x[\"encoder_cat\"])  # returns dictionary with embedding tensors\n",
    "        network_input = torch.cat(\n",
    "            [x[\"encoder_cont\"]]\n",
    "            + [\n",
    "                emb\n",
    "                for name, emb in embeddings.items()\n",
    "                if name in self.encoder_variables or name in self.static_variables\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        prediction = self.network(network_input.view(batch_size, -1))\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction and the target_scale.\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We have used here additional hooks available through the :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` such as ``self.static_variables`` or ``self.encoder_variables`` that can be readily determined from the hyperparameters. See the documentation of the :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` class for all available additions to the :py:class:`~pytorch_forecasting.models.base_model.BaseModel`.\n",
    "\n",
    "When the model receives its input `x`, you can use the hyperparameters and linked to variables and the additional variables by the :py:class:`~pytorch_forecasting.models.base_model.BaseModelWithCovariates` to identify the different variables. This is important as ``x[\"encoder_cat\"].size(2) == x[\"decoder_cat\"].size(2)`` and ``x[\"encoder_cont\"].size(2) == x[\"decoder_cont\"].size(2)``. This means all variables are passed to the encoder and decoder even if some are not allowed to be used by the decoder as they are not known in the future. The order of variables in ``x[\"encoder_cont\"]`` / ``x[\"decoder_cont\"]`` and ``x[\"encoder_cat\"]`` / ``x[\"decoder_cat\"]``is determined by the hyperparameters ``x_reals`` and ``x_categoricals``. Consequently, you can idenify, for example, the position of all continuous decoder variables with ``[self.hparams.x_reals.index(name) for name in self.hparams.time_varying_reals_decoder]``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model does not make use of the known covariates in the decoder - this is obviously suboptimal but not scope of this tutorial. Anyways, let us create a new dataset with categorical variables and see how the model can be instantiated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>group</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>categorical_covariate</th>\n",
       "      <th>real_covariate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007754</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>0.193218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.941164</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>0.572405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.308594</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>0.728801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887074</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>0.023298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.341287</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>b</td>\n",
       "      <td>0.148565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.526935</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>0.563323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.870729</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "      <td>0.255756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.846605</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>b</td>\n",
       "      <td>0.160570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.151567</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "      <td>0.597226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.816219</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>a</td>\n",
       "      <td>0.411941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.494435</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>0.479155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.129647</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>0.798506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.047255</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "      <td>0.170064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.877532</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>0.260794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.889075</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>b</td>\n",
       "      <td>0.937856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.627282</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>b</td>\n",
       "      <td>0.744242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.632390</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>b</td>\n",
       "      <td>0.353051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.233745</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "      <td>0.953005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.336140</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "      <td>0.641806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.237012</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>a</td>\n",
       "      <td>0.594819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.163806</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "      <td>0.136308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.227589</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>0.753491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.807449</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>0.831706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.305031</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>0.765614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.099794</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>0.604094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.621989</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>b</td>\n",
       "      <td>0.207084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.147766</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "      <td>0.807437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.180570</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>b</td>\n",
       "      <td>0.285759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.672017</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>b</td>\n",
       "      <td>0.454346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.339036</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>a</td>\n",
       "      <td>0.559220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       value group  time_idx categorical_covariate  real_covariate\n",
       "0   0.007754     0         0                     a        0.193218\n",
       "1   0.941164     0         1                     a        0.572405\n",
       "2   0.308594     0         2                     a        0.728801\n",
       "3   0.887074     0         3                     a        0.023298\n",
       "4   0.341287     0         4                     b        0.148565\n",
       "5   0.526935     0         5                     a        0.563323\n",
       "6   0.870729     0         6                     a        0.255756\n",
       "7   0.846605     0         7                     b        0.160570\n",
       "8   0.151567     0         8                     a        0.597226\n",
       "9   0.816219     0         9                     a        0.411941\n",
       "10  0.494435     1         0                     b        0.479155\n",
       "11  0.129647     1         1                     b        0.798506\n",
       "12  0.047255     1         2                     b        0.170064\n",
       "13  0.877532     1         3                     a        0.260794\n",
       "14  0.889075     1         4                     b        0.937856\n",
       "15  0.627282     1         5                     b        0.744242\n",
       "16  0.632390     1         6                     b        0.353051\n",
       "17  0.233745     1         7                     a        0.953005\n",
       "18  0.336140     1         8                     a        0.641806\n",
       "19  0.237012     1         9                     a        0.594819\n",
       "20  0.163806     2         0                     b        0.136308\n",
       "21  0.227589     2         1                     b        0.753491\n",
       "22  0.807449     2         2                     a        0.831706\n",
       "23  0.305031     2         3                     a        0.765614\n",
       "24  0.099794     2         4                     a        0.604094\n",
       "25  0.621989     2         5                     b        0.207084\n",
       "26  0.147766     2         6                     a        0.807437\n",
       "27  0.180570     2         7                     b        0.285759\n",
       "28  0.672017     2         8                     b        0.454346\n",
       "29  0.339036     2         9                     a        0.559220"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "test_data_with_covariates = pd.DataFrame(\n",
    "    dict(\n",
    "        # as before\n",
    "        value=np.random.rand(30),\n",
    "        group=np.repeat(np.arange(3), 10),\n",
    "        time_idx=np.tile(np.arange(10), 3),\n",
    "        # now adding covariates\n",
    "        categorical_covariate=np.random.choice([\"a\", \"b\"], size=30),\n",
    "        real_covariate=np.random.rand(30),\n",
    "    )\n",
    ").astype(\n",
    "    dict(group=str)\n",
    ")  # categorical covariates have to be of string type\n",
    "test_data_with_covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                                              | Type                 | Params\n",
      "--------------------------------------------------------------------------------------------\n",
      "0  | loss                                              | SMAPE                | 0     \n",
      "1  | logging_metrics                                   | ModuleList           | 0     \n",
      "2  | input_embeddings                                  | MultiEmbedding       | 11    \n",
      "3  | input_embeddings.embeddings                       | ModuleDict           | 11    \n",
      "4  | input_embeddings.embeddings.group                 | Embedding            | 9     \n",
      "5  | input_embeddings.embeddings.categorical_covariate | Embedding            | 2     \n",
      "6  | network                                           | FullyConnectedModule | 552   \n",
      "7  | network.sequential                                | Sequential           | 552   \n",
      "8  | network.sequential.0                              | Linear               | 310   \n",
      "9  | network.sequential.1                              | ReLU                 | 0     \n",
      "10 | network.sequential.2                              | Linear               | 110   \n",
      "11 | network.sequential.3                              | ReLU                 | 0     \n",
      "12 | network.sequential.4                              | Linear               | 110   \n",
      "13 | network.sequential.5                              | ReLU                 | 0     \n",
      "14 | network.sequential.6                              | Linear               | 22    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"categorical_groups\":                {}\n",
       "\"embedding_labels\":                  {'group': {'0': 0, '1': 1, '2': 2}, 'categorical_covariate': {'a': 0, 'b': 1}}\n",
       "\"embedding_paddings\":                []\n",
       "\"embedding_sizes\":                   {'group': [3, 3], 'categorical_covariate': [2, 1]}\n",
       "\"hidden_size\":                       10\n",
       "\"input_size\":                        5\n",
       "\"learning_rate\":                     0.001\n",
       "\"log_gradient_flow\":                 False\n",
       "\"log_interval\":                      -1\n",
       "\"log_val_interval\":                  -1\n",
       "\"logging_metrics\":                   ModuleList()\n",
       "\"loss\":                              SMAPE()\n",
       "\"monotone_constaints\":               {}\n",
       "\"n_hidden_layers\":                   2\n",
       "\"optimizer\":                         ranger\n",
       "\"output_size\":                       2\n",
       "\"output_transformer\":                GroupNormalizer(transformation='relu')\n",
       "\"reduce_on_plateau_min_lr\":          1e-05\n",
       "\"reduce_on_plateau_patience\":        1000\n",
       "\"static_categoricals\":               ['group']\n",
       "\"static_reals\":                      []\n",
       "\"time_varying_categoricals_decoder\": ['categorical_covariate']\n",
       "\"time_varying_categoricals_encoder\": ['categorical_covariate']\n",
       "\"time_varying_reals_decoder\":        ['real_covariate']\n",
       "\"time_varying_reals_encoder\":        ['real_covariate', 'value']\n",
       "\"weight_decay\":                      0.0\n",
       "\"x_categoricals\":                    ['group', 'categorical_covariate']\n",
       "\"x_reals\":                           ['real_covariate', 'value']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the dataset from the pandas dataframe\n",
    "dataset_with_covariates = TimeSeriesDataSet(\n",
    "    test_data_with_covariates,\n",
    "    group_ids=[\"group\"],\n",
    "    target=\"value\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=5,\n",
    "    max_encoder_length=5,\n",
    "    min_prediction_length=2,\n",
    "    max_prediction_length=2,\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    time_varying_known_reals=[\"real_covariate\"],\n",
    "    time_varying_known_categoricals=[\"categorical_covariate\"],\n",
    "    static_categoricals=[\"group\"],\n",
    ")\n",
    "\n",
    "model = FullyConnectedModelWithCovariates.from_dataset(dataset_with_covariates, hidden_size=10, n_hidden_layers=2)\n",
    "model.summarize(\"full\")  # print model summary\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that the model could be trained, pass a sample batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': tensor([[-0.2650, -0.0057],\n",
       "         [-0.2587, -0.0262],\n",
       "         [-0.2486, -0.0395],\n",
       "         [-0.2714, -0.0140]], grad_fn=<AddmmBackward>),\n",
       " 'target_scale': tensor([[0.4589, 0.2995],\n",
       "         [0.4589, 0.2995],\n",
       "         [0.4589, 0.2995],\n",
       "         [0.4589, 0.2995]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataset_with_covariates.to_dataloader(batch_size=4)))  # generate batch\n",
    "model(x)  # pass batch through model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an autoregressive / recurrent model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Often time series models are autoregressive, i.e. one does not make `n` predictions for all future steps in one function call but predicts ``n`` times one step ahead. PyTorch Forecasting comes with a\n",
    ":py:class:`~pytorch_forecasting.models.base_model.AutoRegressiveBaseModel` and a :py:class:`~pytorch_forecasting.models.base_model.AutoRegressiveBaseModelWithCovariates` for such models.\n",
    "\n",
    ".. autoclass:: pytorch_forecasting.models.base_model.AutoRegressiveBaseModel\n",
    "    :noindex:\n",
    "\n",
    "In this section, we will implement a simple LSTM model that could be easily extended to work with covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type       | Params\n",
      "-----------------------------------------------\n",
      "0 | loss            | SMAPE      | 0     \n",
      "1 | logging_metrics | ModuleList | 0     \n",
      "2 | lstm            | LSTM       | 1 K   \n",
      "3 | output_layer    | Linear     | 11    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"dropout\":                    0.1\n",
       "\"hidden_size\":                10\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       SMAPE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_layers\":                   2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"target\":                     value\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils import rnn\n",
    "\n",
    "from pytorch_forecasting.models.base_model import AutoRegressiveBaseModel\n",
    "\n",
    "\n",
    "class LSTMModel(AutoRegressiveBaseModel):\n",
    "    def __init__(self, target: str, n_layers: int, hidden_size: int, dropout: float = 0.1, **kwargs):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # use pytorch implementation of LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_size=1,\n",
    "            num_layers=self.hparams.n_layers,\n",
    "            dropout=self.hparams.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.hparams.hidden_size, 1)\n",
    "\n",
    "    @property\n",
    "    def target_position(self):\n",
    "        # position of target within reals vector: with covariates: self.hparams.x_reals.index(self.hparams.target)\n",
    "        return 0\n",
    "\n",
    "    def encode(self, x: Dict[str, torch.Tensor]):\n",
    "        # we need at least one encoding step as because the target needs to be lagged by one time step\n",
    "        # as we are lazy, we also require that the encoder length is at least 1, so we can easily generate a\n",
    "        # hidden state here. See the DeepAR implementation for how to use a minimal encoder length of 1\n",
    "        assert x[\"encoder_lengths\"].min() > 2\n",
    "        input_vector = x[\"encoder_cont\"].clone()\n",
    "        # lag target by one\n",
    "        input_vector[..., self.target_position] = torch.roll(input_vector[..., self.target_position], shifts=1, dims=1)\n",
    "        input_vector = input_vector[:, 1:]  # first time step cannot be used because of lagging\n",
    "\n",
    "        # determine effective encoder_length length\n",
    "        effective_encoder_lengths = x[\"encoder_lengths\"] - 1\n",
    "        # run through LSTM network\n",
    "        _, hidden_state = self.lstm(\n",
    "            rnn.pack_padded_sequence(\n",
    "                input_vector, effective_encoder_lengths.cpu(), enforce_sorted=False, batch_first=True\n",
    "            )\n",
    "        )  # second ouput is not needed (hidden state)\n",
    "        return hidden_state\n",
    "\n",
    "    def decode(self, x: Dict[str, torch.Tensor], hidden_state):\n",
    "        # again lag target by one\n",
    "        input_vector = x[\"decoder_cont\"].clone()\n",
    "        input_vector[..., self.target_position] = torch.roll(input_vector[..., self.target_position], shifts=1, dims=1)\n",
    "        # but this time fill in missing target from encoder_cont at the first time step instead of throwing it away\n",
    "        last_encoder_target = x[\"encoder_cont\"][\n",
    "            torch.arange(x[\"encoder_cont\"].size(0)), x[\"encoder_lengths\"] - 1, self.target_position\n",
    "        ]\n",
    "        input_vector[:, 0, self.target_position] = last_encoder_target\n",
    "\n",
    "        if self.training:  # training attribute is provided from PyTorch and indicates if module is in training model\n",
    "            packed_decoder = rnn.pack_padded_sequence(\n",
    "                input_vector, lengths=x[\"decoder_lengths\"].cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            # run through same lstm\n",
    "            lstm_output, _ = self.lstm(packed_decoder, hidden_state)\n",
    "            # unpack sequence\n",
    "            lstm_output, _ = rnn.pad_packed_sequence(lstm_output, batch_first=True)\n",
    "            # transform into right shape\n",
    "            prediction = self.output_layer(lstm_output)\n",
    "\n",
    "        else:  # if not training, need to predict in autoregressive manner\n",
    "            # predict one by one\n",
    "            max_decoder_length = x[\"decoder_lengths\"].max()\n",
    "            # initialize previous target and hidden state\n",
    "            last_target = last_encoder_target\n",
    "            last_hidden_state = hidden_state\n",
    "\n",
    "            predictions = []\n",
    "\n",
    "            # for each time step run prediction\n",
    "            for i in range(max_decoder_length):\n",
    "                current_input_vector = input_vector[:, i].unsqueeze(1)  # select time step in decoder\n",
    "                current_input_vector[:, 0, self.target_position] = last_target  # insert previous target\n",
    "\n",
    "                # make lstm prediction\n",
    "                lstm_prediction, new_hidden_state = self.lstm(current_input_vector, last_hidden_state)\n",
    "                prediction = self.output_layer(lstm_prediction).squeeze(1)\n",
    "\n",
    "                # save prediction\n",
    "                predictions.append(prediction)\n",
    "\n",
    "                # prepare for next time step\n",
    "                last_hidden_state = new_hidden_state\n",
    "\n",
    "                # Prediction should be passed through transformer and then inversely transformed.\n",
    "                # The inverse transformation might be only approximately the inverse of the\n",
    "                # forward transformation making this step important.\n",
    "                rescaled_prediction = self.transform_output(\n",
    "                    dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "                )  # inverse transform\n",
    "                normalized_prediction = self.output_transformer.transform(\n",
    "                    rescaled_prediction, target_scale=x[\"target_scale\"]\n",
    "                )  # transform\n",
    "\n",
    "                last_target = normalized_prediction.squeeze(1)\n",
    "\n",
    "            # stack all predictions\n",
    "            prediction = torch.stack(predictions, dim=1)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        hidden_state = self.encode(x)  # encode to hidden state\n",
    "        prediction = self.decode(x, hidden_state)  # decode leveraging hidden state\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "\n",
    "model = LSTMModel.from_dataset(dataset, n_layers=2, hidden_size=10)\n",
    "model.summarize(\"full\")\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We used the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.transform_output` method to apply the inverse transformation. It is also used under the hood for re-scaling/de-normalizing predictions and leverages the ``output_transformer`` to do so. The ``output_transformer`` is the ``target_normalizer`` as used in the dataset. When initializing the model from the dataset, it is automatically copied to the model.\n",
    "\n",
    "We can now check that both approaches deliver the same result in terms of prediction shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction shape in training: torch.Size([4, 2, 1])\n",
      "prediction shape in inference: torch.Size([4, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "\n",
    "print(\n",
    "    \"prediction shape in training:\", model(x)[\"prediction\"].size()\n",
    ")  # batch_size x decoder time steps x 1 (1 for one target dimension)\n",
    "model.eval()  # set model into eval mode to use autoregressive prediction\n",
    "print(\"prediction shape in inference:\", model(x)[\"prediction\"].size())  # should be the same as in training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using and defining a custom/non-trivial metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a different metric, simply pass it to the model when initializing it (preferably via the `from_dataset()` method). For example, to use mean absolute error with our `FullyConnectedModel` from the beginning of this tutorial, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       MAE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_forecasting.metrics import MAE\n",
    "\n",
    "model = FullyConnectedModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2, loss=MAE())\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some metrics might require a certain form of model prediction, e.g. quantile prediction assumes an output of shape `batch_size x n_decoder_timesteps x n_quantiles` instead of `batch_size x n_decoder_timesteps`. For the `FullyConnectedModel`, this means that we need to use a modified `FullyConnectedModule`network. Here `n_outputs` corresponds to the number of quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2, 7])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class FullyConnectedMultiOutputModule(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, n_outputs: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # input layer\n",
    "        module_list = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden_layers):\n",
    "            module_list.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
    "        # output layer\n",
    "        self.n_outputs = n_outputs\n",
    "        module_list.append(\n",
    "            nn.Linear(hidden_size, output_size * n_outputs)\n",
    "        )  # <<<<<<<< modified: replaced output_size with output_size * n_outputs\n",
    "\n",
    "        self.sequential = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x of shape: batch_size x n_timesteps_in\n",
    "        # output of shape batch_size x n_timesteps_out\n",
    "        return self.sequential(x).reshape(x.size(0), -1, self.n_outputs)  # <<<<<<<< modified: added reshape\n",
    "\n",
    "\n",
    "# test that network works as intended\n",
    "network = FullyConnectedMultiOutputModule(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2, n_outputs=7)\n",
    "x = torch.rand(20, 5)\n",
    "network(x).shape  # <<<<<<<<<< instead of shape (20, 2), returning additional dimension for quantiles"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Using the above-defined ``FullyConnectedMultiOutputModule``, we could create a new model and use :py:class:`~pytorch_forecasting.metrics.QuantileLoss`. Note that you would have to align ``n_outputs`` with the number of quantiles in the :py:class:`~pytorch_forecasting.metrics.QuantileLoss` class either manually or by making use of the `from_dataset()` method. If you want to switch back to a loss on a single output such as for :py:class:`~pytorch_forecasting.metrics.MAE`, simply set the ``n_ouputs=1`` as all PyTorch Forecasting metrics can handle the additional third dimension as long as it is of size 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple case: model output can be readily converted to prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "To implement a new metric, you simply need to inherit from the :py:class:`~pytorch_forecasting.metrics.MultiHorizonMetric` and define the loss function. The :py:class:`~pytorch_forecasting.metrics.MultiHorizonMetric` handles everything from weighting to masking values for you. E.g. the mean absolute error is implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MultiHorizonMetric\n",
    "\n",
    "\n",
    "class MAE(MultiHorizonMetric):\n",
    "    def loss(self, y_pred, target):\n",
    "        loss = (self.to_prediction(y_pred) - target).abs()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "You might notice the :py:meth:`~pytorch_forecasting.metrics.Metric.to_prediction` method.  Generally speaking, it convertes ``y_pred`` to a point-prediction. By default, this means that it removes the third dimension from ``y_pred`` if there is one. For most metrics, this is exactly what you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced case: model ouptut cannot be readily converted to prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Sometimes a networks's ``forward()`` output does not trivially map to a prediction and your ``to_prediction`` needs to be implemented separately. For example, this is the case if you predict the parameters of a distribution as is the case for all classes deriving from :py:class:`~pytorch_forecasting.metrics.DistributionLoss`. In particular, this means that you need to handle training and prediction differently.\n",
    "\n",
    "We will study now the case of the :py:class:`~pytorch_forecasting.metrics.NormalDistributionLoss`. It requires us to predict the ``mean`` and the ``scale`` of the normal distribution. We can do so by leveraging our ``FullyConnectedMultiOutputModule`` class that we used for predicting multiple quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                10\n",
       "\"input_size\":                 5\n",
       "\"learning_rate\":              0.001\n",
       "\"log_gradient_flow\":          False\n",
       "\"log_interval\":               -1\n",
       "\"log_val_interval\":           -1\n",
       "\"logging_metrics\":            ModuleList()\n",
       "\"loss\":                       SMAPE()\n",
       "\"monotone_constaints\":        {}\n",
       "\"n_hidden_layers\":            2\n",
       "\"optimizer\":                  ranger\n",
       "\"output_size\":                2\n",
       "\"output_transformer\":         GroupNormalizer()\n",
       "\"reduce_on_plateau_min_lr\":   1e-05\n",
       "\"reduce_on_plateau_patience\": 1000\n",
       "\"weight_decay\":               0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import copy\n",
    "\n",
    "from pytorch_forecasting.metrics import NormalDistributionLoss\n",
    "\n",
    "\n",
    "class FullyConnectedForDistributionLossModel(BaseModel):  # we inherit the `from_dataset` method\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = FullyConnectedMultiOutputModule(\n",
    "            input_size=self.hparams.input_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "            n_outputs=2,  # <<<<<<<< we predict two outputs for mean and scale of the normal distribution\n",
    "        )\n",
    "        self.loss = NormalDistributionLoss()\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            \"output_size\": dataset.max_prediction_length,\n",
    "            \"input_size\": dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)  # use to pass real hyperparameters and override defaults set by dataset\n",
    "        # example for dataset validation\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "        assert (\n",
    "            len(dataset.time_varying_known_categoricals) == 0\n",
    "            and len(dataset.time_varying_known_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_categoricals) == 0\n",
    "            and len(dataset.static_categoricals) == 0\n",
    "            and len(dataset.static_reals) == 0\n",
    "            and len(dataset.time_varying_unknown_reals) == 1\n",
    "            and dataset.time_varying_unknown_reals[0] == dataset.target\n",
    "        ), \"Only covariate should be the target in 'time_varying_unknown_reals'\"\n",
    "\n",
    "        return super().from_dataset(dataset, **new_kwargs)\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor], n_samples: int = None) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "        prediction = self.network(network_input)  # shape batch_size x n_decoder_steps x 2\n",
    "        if (\n",
    "            self.training or n_samples is None\n",
    "        ):  # training is a PyTorch variable indicating if a module is being trained (tracing gradients) or evaluated\n",
    "            assert n_samples is None, \"We need to predict parameters when training\"\n",
    "            prediction_type = \"parameters\"\n",
    "        else:\n",
    "            # let's sample from our distribution - first we need to scale the parameters to real space\n",
    "            scaled_parameters = self.transform_output(\n",
    "                dict(\n",
    "                    prediction=prediction,\n",
    "                    target_scale=x[\"target_scale\"],\n",
    "                    prediction_type=\"parameters\",\n",
    "                )\n",
    "            )\n",
    "            # and then sample from distribution\n",
    "            prediction = self.loss.sample(scaled_parameters, n_samples)\n",
    "            prediction_type = \"samples\"\n",
    "        return dict(prediction=prediction, target_scale=x[\"target_scale\"], prediction_type=prediction_type)\n",
    "\n",
    "    def transform_output(self, out: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        # input is forward's output\n",
    "        # depending on output, transform differently\n",
    "        if out[\"prediction_type\"] == \"samples\":  # samples are already rescaled\n",
    "            out = out[\"prediction\"]\n",
    "        else:  # parameters need to be rescaled\n",
    "            out = self.loss.rescale_parameters(\n",
    "                out[\"prediction\"], target_scale=out[\"target_scale\"], encoder=self.output_transformer\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    def log_prediction(self, x, out, batch_idx) -> None:\n",
    "        if (\n",
    "            out[\"prediction_type\"] == \"parameters\"\n",
    "            and (batch_idx % self.log_interval == 0 or self.log_interval < 1.0)\n",
    "            and self.log_interval > 0\n",
    "        ):\n",
    "            out = copy(out)  # copy to avoid side-effects but do not deep copy to re-use references\n",
    "            # sample from distribution to create valid prediction\n",
    "            y_hat_detached = out[\"prediction\"].detach()\n",
    "            y_hat_samples = self.loss.sample(y_hat_detached, 100)\n",
    "            out[\"prediction\"] = y_hat_samples\n",
    "            out[\"prediction_type\"] = \"samples\"\n",
    "        super().log_prediction(x, out, batch_idx=batch_idx)\n",
    "\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        x: Dict[str, torch.Tensor],\n",
    "        y: torch.Tensor,\n",
    "        out: Dict[str, torch.Tensor],\n",
    "    ) -> None:\n",
    "        # Metrics (in contrast to the training loss: distribution loss) are calculated based on point predictions.\n",
    "        # Therefore, we need to convert parameter outputs to\n",
    "        if out[\"prediction_type\"] == \"parameters\":\n",
    "            # use distribution properties to create point prediction\n",
    "            out = copy(out)  # copy to avoid side-effects but do not deep copy to re-use references\n",
    "            y_hat_detached = out[\"prediction\"].detach()\n",
    "            y_hat_point_detached = self.loss.map_x_to_distribution(y_hat_detached).mean.unsqueeze(-1)\n",
    "            out[\"prediction\"] = y_hat_point_detached\n",
    "            out[\"prediction_type\"] = \"samples\"\n",
    "        super().log_metrics(x, y, out)\n",
    "\n",
    "\n",
    "model = FullyConnectedForDistributionLossModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2)\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "You notice that had overriden the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.transform_output` method. This method is responsible for rescaling the output of a network into real space. This is often trivial as the normalization of the target variable simply has to be inverted (and this is also the default). In case of distribution loss, a custom version is required.\n",
    "\n",
    "Further, the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.log_metrics` method had to be overridden because the output of the network cannot be readily used for evaluating metrics such as mean absolute error - we first need to convert them into a point prediction - here not by sampling but by analytically calculating the mean of the distribution. The same is necessary for plotting the prediction. Therefore, we need to override :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.log_prediction`. This is it - the network is fully ready for training.\n",
    "\n",
    "We can now test that the network works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter predition shape:  torch.Size([4, 2, 2])\n",
      "sample prediction shape:  torch.Size([4, 2, 200])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "\n",
    "print(\"parameter predition shape: \", model(x)[\"prediction\"].size())\n",
    "model.eval()  # set model into eval mode for sampling\n",
    "print(\"sample prediction shape: \", model(x, n_samples=200)[\"prediction\"].size())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "To run inference, you can still use the :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.predict()` method, as additional arguments are passed to the network's ``forward()`` method, i.e. we can execute the following line to generate 100 traces and subsequently calculate quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2, 7])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(dataloader, n_samples=100, mode=\"quantiles\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned quantiles are here determined by the quantiles defined in the loss function and can be modified by passing a list of quantiles to at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss.quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2, 0.8]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormalDistributionLoss(quantiles=[0.2, 0.8]).quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding custom plotting and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Forecasting supports plotting of predictions and interpretations. The figures can also be logged as part of monitoring training progress using tensorboard. Sometimes, the output of the network cannot be directly plotted together with the actually observed time series. In these cases (such as our `FullyConnectedForDistributionLossModel` from the previous section), we need to fix the plotting function. Further, sometimes we want to visualize certain properties of the network every other batch or after every epoch. It is easy to make this happen with PyTorch Forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal testing of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing models is essential to quickly detect problems and iterate quickly. Some issues can be only identified after lengthy training but many problems show up after one or two batches. PyTorch Lightning, on which PyTorch Forecasting is built, makes it easy to set up such tests."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Every model should be trainable with some minimal dataset. Here is how:\n",
    "\n",
    "#. Define a dataset that works with the model. If it takes long to create, you can save it to disk with the :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.save` method and load it with the :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.load` method when you want to run tests. In any case, create a reasonably small dataset.\n",
    "\n",
    "#. Initialize your model with ``log_interval=1`` to test logging of plots - in particular the `plot_prediction()` method.\n",
    "\n",
    "#. Define a `Pytorch Lightning Trainer <https://pytorch-lightning.readthedocs.io/en/latest/trainer.html>`_ and initialize it with ``fast_dev_run=True``. This ensures that not full epochs but just a couple of batches are passed through the training and validation steps.\n",
    "\n",
    "#. Train your model and check that it executes.\n",
    "\n",
    "As example, we marshall the ``FullyConnectedForDistributionLossModel`` defined earlier in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "\n",
      "  | Name            | Type                            | Params\n",
      "--------------------------------------------------------------------\n",
      "0 | loss            | NormalDistributionLoss          | 0     \n",
      "1 | logging_metrics | ModuleList                      | 0     \n",
      "2 | network         | FullyConnectedMultiOutputModule | 324   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [28:47<?, ?it/s] \n",
      "Epoch 0:  50%|█████     | 1/2 [00:00<00:00,  6.90it/s, loss=-0.155, v_num=9, train_loss_step=-.155]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00,  6.68it/s, loss=-0.155, v_num=9, train_loss_step=-.155, val_loss=0.136]\n",
      "Epoch 0: 100%|██████████| 2/2 [00:00<00:00,  6.53it/s, loss=-0.155, v_num=9, train_loss_step=-.155, val_loss=0.136]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = FullyConnectedForDistributionLossModel.from_dataset(dataset, hidden_size=10, n_hidden_layers=2, log_interval=1)\n",
    "trainer = Trainer(fast_dev_run=True)\n",
    "trainer.fit(model, train_dataloader=dataloader, val_dataloaders=dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python37764bitbaseconda4052e86d6f894f0ea94517897490b6df"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
